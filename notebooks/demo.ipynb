{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adefd61d-1866-4bd1-ad59-5db4d4c1472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SNIPPETS_PATH = Path(\"../data/processed/snippets.npz\")\n",
    "AUTOENC_EMBED_PATH = Path(\"../data/processed/autoencoder_embeddings.npz\")\n",
    "CONTRASTIVE_EMBED_PATH = Path(\"../data/processed/contrastive_embeddings.npz\")\n",
    "\n",
    "def load_embedding_index(embed_npz_path=AUTOENC_EMBED_PATH, snippets_npz_path=SNIPPETS_PATH):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - embeddings: (N, D)\n",
    "      - song_ids: (N,)\n",
    "      - midi_filenames: per-song\n",
    "      - genres: (N,) if present\n",
    "      - snippet_labels, start_secs, end_secs if present\n",
    "      - snippet_length\n",
    "      - min_interval, vocab_size (needed for embedding new MIDI)\n",
    "    \"\"\"\n",
    "    emb_data = np.load(embed_npz_path, allow_pickle=True)\n",
    "    embeddings = emb_data[\"embeddings\"]       # (N, D)\n",
    "    song_ids   = emb_data[\"song_ids\"]         # (N,)\n",
    "    midi_filenames = emb_data.get(\"midi_filenames\", None)\n",
    "    min_interval = int(emb_data[\"min_interval\"])\n",
    "    vocab_size   = int(emb_data[\"vocab_size\"])\n",
    "\n",
    "    snip_data = np.load(snippets_npz_path, allow_pickle=True)\n",
    "    intervals = snip_data[\"intervals\"]        # (N, L)\n",
    "    snippet_length = intervals.shape[1]\n",
    "\n",
    "    genres = snip_data.get(\"genres\", None)\n",
    "    labels = snip_data.get(\"snippet_labels\", None)\n",
    "    start_secs = snip_data.get(\"snippet_start_secs\", None)\n",
    "    end_secs   = snip_data.get(\"snippet_end_secs\", None)\n",
    "\n",
    "    # sanity\n",
    "    assert embeddings.shape[0] == intervals.shape[0] == song_ids.shape[0], \\\n",
    "        \"Embeddings, intervals, and song_ids must have same length.\"\n",
    "\n",
    "    return {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"song_ids\": song_ids,\n",
    "        \"midi_filenames\": midi_filenames,\n",
    "        \"min_interval\": min_interval,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"genres\": genres,\n",
    "        \"labels\": labels,\n",
    "        \"start_secs\": start_secs,\n",
    "        \"end_secs\": end_secs,\n",
    "        \"snippet_length\": snippet_length,\n",
    "    }\n",
    "\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    a: (N, D)\n",
    "    b: (D,)\n",
    "    returns: (N,) cosine similarities\n",
    "    \"\"\"\n",
    "    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = b / np.linalg.norm(b)\n",
    "    return a_norm @ b_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66066898-1dd8-48a0-a323-25cc39ea152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_retrieval_by_snippet_index(\n",
    "    query_idx: int,\n",
    "    top_k: int = 10,\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,   # or CONTRASTIVE_EMBED_PATH\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple demo: pick an existing snippet by index and retrieve nearest neighbors\n",
    "    using the precomputed embeddings. No model needed.\n",
    "    \"\"\"\n",
    "    index = load_embedding_index(embed_npz_path, snippets_npz_path)\n",
    "\n",
    "    embeddings = index[\"embeddings\"]\n",
    "    song_ids   = index[\"song_ids\"]\n",
    "    midi_files = index[\"midi_filenames\"]\n",
    "    genres     = index[\"genres\"]\n",
    "    labels     = index[\"labels\"]\n",
    "    start_secs = index[\"start_secs\"]\n",
    "    end_secs   = index[\"end_secs\"]\n",
    "\n",
    "    N = embeddings.shape[0]\n",
    "    if query_idx < 0 or query_idx >= N:\n",
    "        raise ValueError(f\"query_idx {query_idx} out of range [0, {N-1}]\")\n",
    "\n",
    "    sims = cosine_sim(embeddings, embeddings[query_idx])\n",
    "    sims[query_idx] = -np.inf  # exclude self\n",
    "    sorted_idx = np.argsort(-sims)\n",
    "\n",
    "    print(f\"\\n=== Retrieval demo (existing snippet) ===\")\n",
    "    print(f\"Query snippet index: {query_idx}\")\n",
    "    sid_q = int(song_ids[query_idx])\n",
    "    fname_q = midi_files[sid_q] if midi_files is not None else \"N/A\"\n",
    "    genre_q = genres[query_idx] if genres is not None else \"unknown\"\n",
    "    label_q = labels[query_idx] if labels is not None else f\"snippet_{query_idx}\"\n",
    "\n",
    "    print(f\"  query song_id={sid_q}, genre={genre_q}, file={fname_q}\")\n",
    "    print(f\"  label={label_q}\")\n",
    "    if start_secs is not None and end_secs is not None:\n",
    "        print(f\"  approx time: {start_secs[query_idx]:.2f}s → {end_secs[query_idx]:.2f}s\")\n",
    "\n",
    "    print(f\"\\nTop {top_k} neighbors:\")\n",
    "    for rank, idx in enumerate(sorted_idx[:top_k], start=1):\n",
    "        sid = int(song_ids[idx])\n",
    "        fname = midi_files[sid] if midi_files is not None else \"N/A\"\n",
    "        genre = genres[idx] if genres is not None else \"unknown\"\n",
    "        label = labels[idx] if labels is not None else f\"snippet_{idx}\"\n",
    "        print(f\"#{rank:02d}  sim={sims[idx]:.3f}\")\n",
    "        print(f\"     idx={idx}, song_id={sid}, genre={genre}, file={fname}\")\n",
    "        print(f\"     label={label}\")\n",
    "        if start_secs is not None and end_secs is not None:\n",
    "            print(f\"     approx time: {start_secs[idx]:.2f}s → {end_secs[idx]:.2f}s\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45642770-8f0f-404d-8c1b-03cbc1e8f2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieval demo (existing snippet) ===\n",
      "Query snippet index: 201\n",
      "  query song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "  label=classic_Axel_F_1_idx000000_to000032_t0000.00s_to0027.26s\n",
      "  approx time: 0.00s → 27.26s\n",
      "\n",
      "Top 10 neighbors:\n",
      "#01  sim=1.000\n",
      "     idx=204, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000048_to000080_t0040.90s_to0068.16s\n",
      "     approx time: 40.90s → 68.16s\n",
      "\n",
      "#02  sim=0.869\n",
      "     idx=28179, song_id=749, genre=rnb, file=Feel_So_High.mid\n",
      "     label=rnb_Feel_So_High_idx000192_to000224_t0088.79s_to0104.59s\n",
      "     approx time: 88.79s → 104.59s\n",
      "\n",
      "#03  sim=0.835\n",
      "     idx=26177, song_id=704, genre=pop, file=When the Going Gets Tough.mid\n",
      "     label=pop_When the Going Gets Tough_idx000016_to000048_t0008.73s_to0028.51s\n",
      "     approx time: 8.73s → 28.51s\n",
      "\n",
      "#04  sim=0.834\n",
      "     idx=207, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000096_to000128_t0081.79s_to0121.64s\n",
      "     approx time: 81.79s → 121.64s\n",
      "\n",
      "#05  sim=0.824\n",
      "     idx=2484, song_id=63, genre=classic, file=addictedtolove.mid\n",
      "     label=classic_addictedtolove_idx000240_to000272_t0112.05s_to0125.08s\n",
      "     approx time: 112.05s → 125.08s\n",
      "\n",
      "#06  sim=0.821\n",
      "     idx=25238, song_id=678, genre=pop, file=Outstanding.mid\n",
      "     label=pop_Outstanding_idx000400_to000432_t0248.22s_to0268.82s\n",
      "     approx time: 248.22s → 268.82s\n",
      "\n",
      "#07  sim=0.819\n",
      "     idx=12834, song_id=352, genre=folk, file=letyourloveflow.mid\n",
      "     label=folk_letyourloveflow_idx000128_to000160_t0090.52s_to0114.66s\n",
      "     approx time: 90.52s → 114.66s\n",
      "\n",
      "#08  sim=0.818\n",
      "     idx=25213, song_id=678, genre=pop, file=Outstanding.mid\n",
      "     label=pop_Outstanding_idx000000_to000032_t0000.00s_to0012.69s\n",
      "     approx time: 0.00s → 12.69s\n",
      "\n",
      "#09  sim=0.815\n",
      "     idx=16467, song_id=449, genre=guitar, file=new_york_state_of_mind_dwb.mid\n",
      "     label=guitar_new_york_state_of_mind_dwb_idx000080_to000112_t0069.29s_to0100.98s\n",
      "     approx time: 69.29s → 100.98s\n",
      "\n",
      "#10  sim=0.812\n",
      "     idx=28734, song_id=762, genre=rnb, file=Im Your Angel.mid\n",
      "     label=rnb_Im Your Angel_idx000256_to000288_t0207.16s_to0237.24s\n",
      "     approx time: 207.16s → 237.24s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_retrieval_by_snippet_index(\n",
    "    query_idx=201,\n",
    "    top_k=10,\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,  # or CONTRASTIVE_EMBED_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "911d494c-a6e7-420c-bb11-168e3a46f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "from typing import List, Tuple\n",
    "from music21 import converter, instrument, note, chord, stream, key, interval, pitch\n",
    "\n",
    "# how many notes per snippet (you can tweak)\n",
    "SNIPPET_LENGTH = 32\n",
    "\n",
    "# base rhythmic unit: 1 quarter note = 4 steps, so 1 step = sixteenth note\n",
    "STEPS_PER_QUARTER = 4\n",
    "\n",
    "\n",
    "def load_midi(filepath: Path) -> stream.Score:\n",
    "    \"\"\"Load a MIDI file into a music21 Score.\"\"\"\n",
    "    return converter.parse(str(filepath))\n",
    "\n",
    "\n",
    "def pick_melody_part(score: stream.Score) -> stream.Part | None:\n",
    "    \"\"\"\n",
    "    Heuristic for picking the 'melody' part:\n",
    "\n",
    "    1. Skip *purely* percussion parts.\n",
    "    2. If any part name/instrument name suggests 'melody/lead/right hand',\n",
    "       pick that directly.\n",
    "    3. Otherwise:\n",
    "       - For each remaining part, compute:\n",
    "         * n_notes\n",
    "         * avg_pitch\n",
    "       - Compute median avg_pitch across candidates.\n",
    "       - Filter to parts with avg_pitch >= median (favor higher voices).\n",
    "       - Among those, pick the one with the most notes; break ties by higher avg_pitch.\n",
    "\n",
    "    Returns the chosen Part, or None if nothing suitable is found.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    for p in score.parts:\n",
    "        insts = list(p.getInstruments())\n",
    "\n",
    "        # Determine if this part is purely percussion (all instruments percussion-like)\n",
    "        has_percussion = any(\n",
    "            isinstance(i, instrument.UnpitchedPercussion) or\n",
    "            (\"percussion\" in (i.bestName() or \"\").lower())\n",
    "            for i in insts\n",
    "        )\n",
    "        has_non_percussion = any(\n",
    "            not isinstance(i, instrument.UnpitchedPercussion) and\n",
    "            \"percussion\" not in (i.bestName() or \"\").lower()\n",
    "            for i in insts\n",
    "        )\n",
    "\n",
    "        # Skip only if it's *purely* percussion, not mixed\n",
    "        if has_percussion and not has_non_percussion:\n",
    "            continue\n",
    "\n",
    "        # Collect notes/chords\n",
    "        notes_chords = [n for n in p.recurse().notes if isinstance(n, (note.Note, chord.Chord))]\n",
    "        if not notes_chords:\n",
    "            continue\n",
    "\n",
    "        # Basic stats\n",
    "        pitches = []\n",
    "        for n in notes_chords:\n",
    "            if isinstance(n, note.Note):\n",
    "                pitches.append(n.pitch.midi)\n",
    "            elif isinstance(n, chord.Chord):\n",
    "                pitches.append(max(nn.pitch.midi for nn in n.notes))\n",
    "\n",
    "        if not pitches:\n",
    "            continue\n",
    "\n",
    "        n_notes = len(pitches)\n",
    "        avg_pitch = sum(pitches) / len(pitches)\n",
    "\n",
    "        # part/instrument names (lowercased)\n",
    "        part_name = (p.partName or \"\").lower()\n",
    "        inst_names = [str(inst.instrumentName or \"\").lower()\n",
    "                      for inst in insts]\n",
    "\n",
    "        candidates.append({\n",
    "            \"part\": p,\n",
    "            \"n_notes\": n_notes,\n",
    "            \"avg_pitch\": avg_pitch,\n",
    "            \"part_name\": part_name,\n",
    "            \"inst_names\": inst_names,\n",
    "        })\n",
    "\n",
    "    if not candidates:\n",
    "        print(\"  [warn] no suitable melodic parts; skipping this file.\")\n",
    "        return None\n",
    "\n",
    "    # 1) Name-based shortcut: if any part name/instrument suggests \"melody\"\n",
    "    name_keywords = [\n",
    "        \"melody\", \"lead\", \"right hand\", \"rh\", \"treble\", \"solo\", \"violin\", \"flute\", \"trumpet\", \"saw wave\"\n",
    "    ]\n",
    "\n",
    "    def looks_like_melody(c):\n",
    "        text = c[\"part_name\"] + \" \" + \" \".join(c[\"inst_names\"])\n",
    "        text = text.lower()\n",
    "        return any(kw in text for kw in name_keywords)\n",
    "\n",
    "    name_candidates = [c for c in candidates if looks_like_melody(c)]\n",
    "    if name_candidates:\n",
    "        # among these, pick the one with highest avg_pitch (just in case)\n",
    "        best = max(name_candidates, key=lambda c: c[\"avg_pitch\"])\n",
    "        print(f\"  [info] pick_melody_part: selected by name heuristic: \"\n",
    "              f\"part_name='{best['part_name']}', avg_pitch={best['avg_pitch']:.1f}, n_notes={best['n_notes']}\")\n",
    "        return best[\"part\"]\n",
    "\n",
    "    # 2) Pitch-based filtering: keep only parts at or above median avg_pitch\n",
    "    avg_pitches = [c[\"avg_pitch\"] for c in candidates]\n",
    "    median_pitch = sorted(avg_pitches)[len(avg_pitches) // 2]\n",
    "\n",
    "    high_voice_candidates = [c for c in candidates if c[\"avg_pitch\"] >= median_pitch]\n",
    "    if not high_voice_candidates:\n",
    "        high_voice_candidates = candidates  # fallback to all\n",
    "\n",
    "    # 3) Among high-voice candidates, pick the one with the most notes & higher pitch\n",
    "    best = max(\n",
    "        high_voice_candidates,\n",
    "        key=lambda c: (c[\"n_notes\"], c[\"avg_pitch\"])  # primary: many notes, secondary: higher pitch\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  [info] pick_melody_part: selected by stats: \"\n",
    "        f\"part_name='{best['part_name']}', avg_pitch={best['avg_pitch']:.1f}, \"\n",
    "        f\"n_notes={best['n_notes']}\"\n",
    "    )\n",
    "\n",
    "    return best[\"part\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_key_and_transpose(melody: stream.Part) -> stream.Part:\n",
    "    \"\"\"\n",
    "    Detect key with music21 and transpose so tonic is C (for major) or A (for minor).\n",
    "    If key detection fails for some reason, return the original melody.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        key_guess = melody.analyze('key')\n",
    "    except Exception as e:\n",
    "        print(\"  [warn] key analysis failed, leaving melody untransposed:\", e)\n",
    "        return melody\n",
    "\n",
    "    # Decide target tonic\n",
    "    if key_guess.mode == 'major':\n",
    "        target_pitch = pitch.Pitch('C')\n",
    "    else:\n",
    "        # treat minor keys as aiming for A minor tonic\n",
    "        target_pitch = pitch.Pitch('A')\n",
    "\n",
    "    # Build interval from current tonic to target tonic\n",
    "    itvl = interval.Interval(key_guess.tonic, target_pitch)\n",
    "\n",
    "    transposed = melody.transpose(itvl)\n",
    "    return transposed\n",
    "\n",
    "\n",
    "def extract_pitch_duration_sequence(melody: stream.Part) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Extract (midi_pitch, quarter_length_duration) from a melody line.\n",
    "    Ignore rests; collapse chords to their top note.\n",
    "    \"\"\"\n",
    "    seq = []\n",
    "    for elem in melody.recurse().notesAndRests:\n",
    "        if isinstance(elem, note.Note):\n",
    "            midi_pitch = elem.pitch.midi\n",
    "            dur = float(elem.quarterLength)\n",
    "            seq.append((midi_pitch, dur))\n",
    "        elif isinstance(elem, chord.Chord):\n",
    "            # take highest note in chord as melody approximation\n",
    "            midi_pitch = max(n.pitch.midi for n in elem.notes)\n",
    "            dur = float(elem.quarterLength)\n",
    "            seq.append((midi_pitch, dur))\n",
    "        else:\n",
    "            # ignore rests and other stuff for now\n",
    "            continue\n",
    "    return seq\n",
    "\n",
    "\n",
    "def convert_to_intervals_and_durations(\n",
    "    pitch_dur_seq: List[Tuple[int, float]]\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Convert absolute pitches to pitch intervals and durations to integer steps.\n",
    "    intervals[i] = pitch[i] - pitch[i-1], with first interval = 0\n",
    "    durations[i] = round( quarter_length * STEPS_PER_QUARTER )\n",
    "    \"\"\"\n",
    "    if not pitch_dur_seq:\n",
    "        return [], []\n",
    "\n",
    "    pitches = [p for (p, _) in pitch_dur_seq]\n",
    "    durs_q = [d for (_, d) in pitch_dur_seq]\n",
    "\n",
    "    intervals = [0]  # first note has no previous reference\n",
    "    for i in range(1, len(pitches)):\n",
    "        intervals.append(int(pitches[i] - pitches[i - 1]))\n",
    "\n",
    "    durations = [max(1, int(round(d * STEPS_PER_QUARTER))) for d in durs_q]\n",
    "\n",
    "    return intervals, durations\n",
    "\n",
    "\n",
    "def make_snippets(\n",
    "    intervals: List[int],\n",
    "    durations: List[int],\n",
    "    snippet_length: int = SNIPPET_LENGTH\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Slice sequences into fixed-length snippets.\n",
    "    We use a simple sliding window with stride = snippet_length // 2 (50% overlap).\n",
    "    Short sequences yield zero snippets.\n",
    "    \"\"\"\n",
    "    assert len(intervals) == len(durations)\n",
    "    n = len(intervals)\n",
    "    if n < snippet_length:\n",
    "        return np.empty((0, snippet_length), dtype=np.int32), np.empty((0, snippet_length), dtype=np.int32)\n",
    "\n",
    "    stride = snippet_length // 2\n",
    "    interval_snips = []\n",
    "    duration_snips = []\n",
    "\n",
    "    for start in range(0, n - snippet_length + 1, stride):\n",
    "        end = start + snippet_length\n",
    "        interval_snips.append(intervals[start:end])\n",
    "        duration_snips.append(durations[start:end])\n",
    "\n",
    "    return np.array(interval_snips, dtype=np.int32), np.array(duration_snips, dtype=np.int32)\n",
    "\n",
    "def make_snippets_with_timestamps(\n",
    "    intervals: List[int],\n",
    "    durations: List[int],\n",
    "    durations_q: List[float],\n",
    "    snippet_length: int = SNIPPET_LENGTH\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Slice sequences into fixed-length snippets.\n",
    "    Also return timestamp pairs (start_q, end_q) in quarter lengths.\n",
    "    \"\"\"\n",
    "    assert len(intervals) == len(durations)\n",
    "    n = len(intervals)\n",
    "    if n < snippet_length:\n",
    "        return np.empty((0, snippet_length), dtype=np.int32), np.empty((0, snippet_length), dtype=np.int32), []\n",
    "\n",
    "    stride = snippet_length // 2\n",
    "    interval_snips, duration_snips, timestamps = [], [], []\n",
    "\n",
    "    cumulative_q = np.cumsum([0] + durations_q)  # cumulative time in quarter lengths\n",
    "\n",
    "    for start in range(0, n - snippet_length + 1, stride):\n",
    "        end = start + snippet_length\n",
    "        interval_snips.append(intervals[start:end])\n",
    "        duration_snips.append(durations[start:end])\n",
    "\n",
    "        start_q = cumulative_q[start]\n",
    "        end_q = cumulative_q[end]\n",
    "        timestamps.append((int(round(start_q)), int(round(end_q))))\n",
    "\n",
    "    return np.array(interval_snips, dtype=np.int32), np.array(duration_snips, dtype=np.int32), timestamps\n",
    "\n",
    "from music21 import instrument\n",
    "\n",
    "def sanitize_melody_instrument(melody_part):\n",
    "    \"\"\"\n",
    "    Remove any existing Instrument metadata and force a clean, non-percussion\n",
    "    instrument on a non-drum channel.\n",
    "    \"\"\"\n",
    "    # Remove ALL Instrument objects from this part\n",
    "    for inst in list(melody_part.recurse().getElementsByClass(instrument.Instrument)):\n",
    "        try:\n",
    "            melody_part.remove(inst)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Set a friendly part name\n",
    "    melody_part.partName = \"Melody\"\n",
    "\n",
    "    # Insert one clean Piano instrument at the beginning\n",
    "    piano = instrument.Piano()\n",
    "    piano.midiProgram = 0  # Acoustic Grand\n",
    "    piano.midiChannel = 0  # Channel 1 (NOT 10/drums)\n",
    "    melody_part.insert(0, piano)\n",
    "\n",
    "    return melody_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48843c9a-dad2-445b-9539-651ab7b5e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_midi_file_to_snippets(\n",
    "    midi_path,\n",
    "    model,\n",
    "    min_interval: int,\n",
    "    snippet_length: int,\n",
    "    device=None,\n",
    "    model_type: str = \"auto\",  # \"auto\" or \"contrastive\"\n",
    "):\n",
    "    \"\"\"\n",
    "    midi_path: path to a MIDI file\n",
    "    model: trained encoder (autoencoder or contrastive)\n",
    "    min_interval: int, used to shift intervals into token IDs\n",
    "    snippet_length: int, must match training\n",
    "    model_type: \"auto\" (call model.encode) or \"contrastive\" (call model(x))\n",
    "\n",
    "    Returns:\n",
    "        query_embs: (Q, D) numpy array, one embedding per snippet from this MIDI\n",
    "    \"\"\"\n",
    "    midi_path = Path(midi_path)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 1) Load score + pick melody\n",
    "    score = load_midi(midi_path)\n",
    "    melody = pick_melody_part(score)\n",
    "    if melody is None:\n",
    "        raise ValueError(f\"No usable melody part found in {midi_path.name}\")\n",
    "\n",
    "    # 2) Transpose + sanitize (same as preprocessing)\n",
    "    melody = detect_key_and_transpose(melody)\n",
    "    melody = sanitize_melody_instrument(melody)\n",
    "\n",
    "    # 3) Extract (pitch, duration) sequence\n",
    "    pitch_dur_seq = extract_pitch_duration_sequence(melody)\n",
    "    if len(pitch_dur_seq) < snippet_length:\n",
    "        raise ValueError(\n",
    "            f\"Melody has only {len(pitch_dur_seq)} events < snippet_length={snippet_length}\"\n",
    "        )\n",
    "\n",
    "    # 4) Convert to intervals + integer duration steps\n",
    "    intervals, durations_steps = convert_to_intervals_and_durations(pitch_dur_seq)\n",
    "\n",
    "    # 5) Slice into fixed-length snippets\n",
    "    i_snips, d_snips = make_snippets(intervals, durations_steps, snippet_length)\n",
    "    if i_snips.shape[0] == 0:\n",
    "        raise ValueError(\"No snippets extracted from this MIDI (after slicing).\")\n",
    "\n",
    "    # 6) Shift intervals to token IDs\n",
    "    shifted_snips = i_snips - min_interval  # (Q, L)\n",
    "\n",
    "    # 7) Encode\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(shifted_snips, dtype=torch.long, device=device)\n",
    "        if model_type == \"auto\":\n",
    "            z = model.encode(x)   # (Q, D)\n",
    "        else:\n",
    "            z = model(x)         # (Q, D) for contrastive encoder\n",
    "\n",
    "        query_embs = z.cpu().numpy()\n",
    "\n",
    "    return query_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a4861c7-e652-4596-a7f1-7d14c05f501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_retrieval_for_midi(\n",
    "    midi_path,\n",
    "    model,\n",
    "    model_type: str = \"auto\",   # \"auto\" or \"contrastive\"\n",
    "    top_k: int = 10,\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,   # or CONTRASTIVE_EMBED_PATH\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a new MIDI, embed it using a trained encoder and retrieve similar snippets.\n",
    "    `model` is your already-trained model object in this notebook.\n",
    "    \"\"\"\n",
    "    index = load_embedding_index(embed_npz_path, snippets_npz_path)\n",
    "\n",
    "    embeddings = index[\"embeddings\"]\n",
    "    song_ids   = index[\"song_ids\"]\n",
    "    midi_files = index[\"midi_filenames\"]\n",
    "    min_interval = index[\"min_interval\"]\n",
    "    vocab_size   = index[\"vocab_size\"]\n",
    "    genres     = index[\"genres\"]\n",
    "    labels     = index[\"labels\"]\n",
    "    start_secs = index[\"start_secs\"]\n",
    "    end_secs   = index[\"end_secs\"]\n",
    "    snippet_length = index[\"snippet_length\"]\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\nEmbedding query MIDI: {Path(midi_path).name}\")\n",
    "    query_snip_embs = embed_midi_file_to_snippets(\n",
    "        midi_path=midi_path,\n",
    "        model=model,\n",
    "        min_interval=min_interval,\n",
    "        snippet_length=snippet_length,\n",
    "        device=device,\n",
    "        model_type=model_type,\n",
    "    )\n",
    "    print(f\"  Extracted {query_snip_embs.shape[0]} snippets from query MIDI.\")\n",
    "\n",
    "    # Simple: average to get one song-level embedding\n",
    "    q_emb = query_snip_embs.mean(axis=0)  # (D,)\n",
    "    sims = cosine_sim(embeddings, q_emb)\n",
    "    sorted_idx = np.argsort(-sims)\n",
    "\n",
    "    print(f\"\\n=== Retrieval demo for MIDI: {Path(midi_path).name} ===\")\n",
    "    print(f\"Top {top_k} similar snippets in the library:\\n\")\n",
    "    for rank, idx in enumerate(sorted_idx[:top_k], start=1):\n",
    "        sid = int(song_ids[idx])\n",
    "        fname = midi_files[sid] if midi_files is not None else \"N/A\"\n",
    "        genre = genres[idx] if genres is not None else \"unknown\"\n",
    "        label = labels[idx] if labels is not None else f\"snippet_{idx}\"\n",
    "        print(f\"#{rank:02d}  sim={sims[idx]:.3f}\")\n",
    "        print(f\"     idx={idx}, song_id={sid}, genre={genre}, file={fname}\")\n",
    "        print(f\"     label={label}\")\n",
    "        if start_secs is not None and end_secs is not None:\n",
    "            print(f\"     approx time: {start_secs[idx]:.2f}s → {end_secs[idx]:.2f}s\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "081c1be1-51c9-4eb1-8e4a-5263601af8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# ---------------------\n",
    "# Autoencoder model\n",
    "# ---------------------\n",
    "\n",
    "class MelodyAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder_rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder_rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.output_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Learned start token for the decoder\n",
    "        self.start_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def encode(self, x):\n",
    "        emb = self.embed(x)\n",
    "        _, h_n = self.encoder_rnn(emb)\n",
    "        return h_n[-1]  # (B, H)\n",
    "\n",
    "    def decode(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        z: (B, H)\n",
    "        seq_len: int (L)\n",
    "        Decoder only gets z + a learned start vector, not the target tokens.\n",
    "        \"\"\"\n",
    "        B = z.size(0)\n",
    "        h0 = z.unsqueeze(0)              # (1, B, H)\n",
    "\n",
    "        # Repeat a learned start embedding L times as input\n",
    "        # shape: (B, L, E)\n",
    "        start_emb = self.start_token.expand(B, seq_len, -1)\n",
    "\n",
    "        out, _ = self.decoder_rnn(start_emb, h0)  # (B, L, H)\n",
    "        logits = self.output_fc(out)              # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        L = x.shape[1]\n",
    "        logits = self.decode(z, L)\n",
    "        return logits, z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5407939b-6acf-4cc0-95b4-6754e07605bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MelodyEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, proj_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder_rnn = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        # hidden_dim * 2 because bidirectional\n",
    "        self.proj = nn.Linear(hidden_dim * 2, proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L) token ids\n",
    "        returns: (B, D) L2-normalized embedding\n",
    "        \"\"\"\n",
    "        emb = self.token_embed(x)           # (B, L, E)\n",
    "        _, h_n = self.encoder_rnn(emb)      # (2*num_layers, B, H)\n",
    "        h_fw = h_n[-2]                      # (B, H)\n",
    "        h_bw = h_n[-1]                      # (B, H)\n",
    "        h_cat = torch.cat([h_fw, h_bw], dim=-1)  # (B, 2H)\n",
    "        z = self.proj(h_cat)               # (B, D)\n",
    "        z = F.normalize(z, dim=-1)         # L2-normalize for cosine similarity\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30b27a13-354e-443e-823c-bca63140acf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MelodyEncoder(\n",
       "  (token_embed): Embedding(184, 128)\n",
       "  (encoder_rnn): GRU(128, 256, batch_first=True, bidirectional=True)\n",
       "  (proj): Linear(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load min_interval & vocab_size from any embedding NPZ\n",
    "idx_info = load_embedding_index(AUTOENC_EMBED_PATH, SNIPPETS_PATH)\n",
    "vocab_size = idx_info[\"vocab_size\"]\n",
    "\n",
    "# 1) Autoencoder\n",
    "auto_model = MelodyAutoencoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=64,    # SAME values used during training\n",
    "    hidden_dim=128,\n",
    ")\n",
    "\n",
    "auto_model.load_state_dict(torch.load(\"../models/autoencoder.pt\", map_location=device))\n",
    "auto_model.to(device)\n",
    "auto_model.eval()\n",
    "\n",
    "# 2) Contrastive encoder\n",
    "contrastive_model = MelodyEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,    # same as training\n",
    "    hidden_dim=256 ,\n",
    "    proj_dim=128,\n",
    "    num_layers=1,\n",
    ")\n",
    "\n",
    "contrastive_model.load_state_dict(torch.load(\"../models/contrastive_encoder.pt\", map_location=device))\n",
    "contrastive_model.to(device)\n",
    "contrastive_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ec9dbda-b9b8-473f-bf28-3522f6b5ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding query MIDI: snippet_201_song4.mid\n",
      "  [info] pick_melody_part: selected by stats: part_name='', avg_pitch=61.5, n_notes=33\n",
      "  Extracted 1 snippets from query MIDI.\n",
      "\n",
      "=== Retrieval demo for MIDI: snippet_201_song4.mid ===\n",
      "Top 10 similar snippets in the library:\n",
      "\n",
      "#01  sim=0.957\n",
      "     idx=201, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000000_to000032_t0000.00s_to0027.26s\n",
      "     approx time: 0.00s → 27.26s\n",
      "\n",
      "#02  sim=0.957\n",
      "     idx=204, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000048_to000080_t0040.90s_to0068.16s\n",
      "     approx time: 40.90s → 68.16s\n",
      "\n",
      "#03  sim=0.871\n",
      "     idx=207, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000096_to000128_t0081.79s_to0121.64s\n",
      "     approx time: 81.79s → 121.64s\n",
      "\n",
      "#04  sim=0.833\n",
      "     idx=28179, song_id=749, genre=rnb, file=Feel_So_High.mid\n",
      "     label=rnb_Feel_So_High_idx000192_to000224_t0088.79s_to0104.59s\n",
      "     approx time: 88.79s → 104.59s\n",
      "\n",
      "#05  sim=0.823\n",
      "     idx=1763, song_id=49, genre=classic, file=Take_The_Money_And_Run.mid\n",
      "     label=classic_Take_The_Money_And_Run_idx000096_to000128_t0050.67s_to0068.48s\n",
      "     approx time: 50.67s → 68.48s\n",
      "\n",
      "#06  sim=0.823\n",
      "     idx=18855, song_id=515, genre=hip-hop, file=T'appartengo.mid\n",
      "     label=hip-hop_T'appartengo_idx000336_to000368_t0127.85s_to0140.02s\n",
      "     approx time: 127.85s → 140.02s\n",
      "\n",
      "#07  sim=0.820\n",
      "     idx=34131, song_id=898, genre=rock, file=israels_son.mid\n",
      "     label=rock_israels_son_idx000160_to000192_t0063.34s_to0075.83s\n",
      "     approx time: 63.34s → 75.83s\n",
      "\n",
      "#08  sim=0.816\n",
      "     idx=2484, song_id=63, genre=classic, file=addictedtolove.mid\n",
      "     label=classic_addictedtolove_idx000240_to000272_t0112.05s_to0125.08s\n",
      "     approx time: 112.05s → 125.08s\n",
      "\n",
      "#09  sim=0.816\n",
      "     idx=26177, song_id=704, genre=pop, file=When the Going Gets Tough.mid\n",
      "     label=pop_When the Going Gets Tough_idx000016_to000048_t0008.73s_to0028.51s\n",
      "     approx time: 8.73s → 28.51s\n",
      "\n",
      "#10  sim=0.815\n",
      "     idx=1554, song_id=41, genre=classic, file=Rosana El Talismßn 4.mid\n",
      "     label=classic_Rosana El Talismßn 4_idx000336_to000368_t0131.42s_to0146.81s\n",
      "     approx time: 131.42s → 146.81s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# after training:\n",
    "# result = train_autoencoder(...)\n",
    "# auto_model = result[\"model\"]   # or whatever variable you kept\n",
    "\n",
    "demo_midi_path = \"../data/demo/snippet_201_song4.mid\"  # change this\n",
    "\n",
    "demo_retrieval_for_midi(\n",
    "    midi_path=demo_midi_path,\n",
    "    model=auto_model,\n",
    "    model_type=\"auto\",\n",
    "    top_k=10,\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c3102ed-e9be-4d6d-a92a-3ef9004b7c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding query MIDI: snippet_201_song4.mid\n",
      "  [info] pick_melody_part: selected by stats: part_name='', avg_pitch=61.5, n_notes=33\n",
      "  Extracted 1 snippets from query MIDI.\n",
      "\n",
      "=== Retrieval demo for MIDI: snippet_201_song4.mid ===\n",
      "Top 10 similar snippets in the library:\n",
      "\n",
      "#01  sim=0.927\n",
      "     idx=201, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000000_to000032_t0000.00s_to0027.26s\n",
      "     approx time: 0.00s → 27.26s\n",
      "\n",
      "#02  sim=0.927\n",
      "     idx=204, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000048_to000080_t0040.90s_to0068.16s\n",
      "     approx time: 40.90s → 68.16s\n",
      "\n",
      "#03  sim=0.883\n",
      "     idx=207, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000096_to000128_t0081.79s_to0121.64s\n",
      "     approx time: 81.79s → 121.64s\n",
      "\n",
      "#04  sim=0.785\n",
      "     idx=205, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000064_to000096_t0054.00s_to0081.79s\n",
      "     approx time: 54.00s → 81.79s\n",
      "\n",
      "#05  sim=0.785\n",
      "     idx=202, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000016_to000048_t0013.11s_to0040.90s\n",
      "     approx time: 13.11s → 40.90s\n",
      "\n",
      "#06  sim=0.772\n",
      "     idx=208, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000112_to000144_t0109.06s_to0130.03s\n",
      "     approx time: 109.06s → 130.03s\n",
      "\n",
      "#07  sim=0.757\n",
      "     idx=203, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000032_to000064_t0027.26s_to0054.00s\n",
      "     approx time: 27.26s → 54.00s\n",
      "\n",
      "#08  sim=0.713\n",
      "     idx=32870, song_id=867, genre=rock, file=Pet.mid\n",
      "     label=rock_Pet_idx000144_to000176_t0132.05s_to0166.40s\n",
      "     approx time: 132.05s → 166.40s\n",
      "\n",
      "#09  sim=0.709\n",
      "     idx=212, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000176_to000208_t0146.81s_to0173.03s\n",
      "     approx time: 146.81s → 173.03s\n",
      "\n",
      "#10  sim=0.707\n",
      "     idx=1438, song_id=36, genre=classic, file=Poison Arrow.mid\n",
      "     label=classic_Poison Arrow_idx000256_to000288_t0151.69s_to0175.72s\n",
      "     approx time: 151.69s → 175.72s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# after training:\n",
    "# info_contrastive = train_contrastive_encoder(...)\n",
    "# contrastive_model = info_contrastive[\"model\"]\n",
    "\n",
    "demo_midi_path = \"../data/demo/snippet_201_song4.mid\"\n",
    "\n",
    "demo_retrieval_for_midi(\n",
    "    midi_path=demo_midi_path,\n",
    "    model=contrastive_model,\n",
    "    model_type=\"contrastive\",\n",
    "    top_k=10,\n",
    "    embed_npz_path=CONTRASTIVE_EMBED_PATH,\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d92e6-14da-4ddd-a21f-1f75c04df5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
