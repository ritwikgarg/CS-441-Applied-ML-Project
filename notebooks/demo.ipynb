{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adefd61d-1866-4bd1-ad59-5db4d4c1472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SNIPPETS_PATH = Path(\"../data/processed/snippets.npz\")\n",
    "AUTOENC_EMBED_PATH = Path(\"../data/processed/autoencoder_embeddings.npz\")\n",
    "CONTRASTIVE_EMBED_PATH = Path(\"../data/processed/contrastive_embeddings.npz\")\n",
    "\n",
    "def load_embedding_index(embed_npz_path=AUTOENC_EMBED_PATH, snippets_npz_path=SNIPPETS_PATH):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - embeddings: (N, D)\n",
    "      - song_ids: (N,)\n",
    "      - midi_filenames: per-song\n",
    "      - genres: (N,) if present\n",
    "      - snippet_labels, start_secs, end_secs if present\n",
    "      - snippet_length\n",
    "      - min_interval, vocab_size (needed for embedding new MIDI)\n",
    "    \"\"\"\n",
    "    emb_data = np.load(embed_npz_path, allow_pickle=True)\n",
    "    embeddings = emb_data[\"embeddings\"]       # (N, D)\n",
    "    song_ids   = emb_data[\"song_ids\"]         # (N,)\n",
    "    midi_filenames = emb_data.get(\"midi_filenames\", None)\n",
    "    min_interval = int(emb_data[\"min_interval\"])\n",
    "    vocab_size   = int(emb_data[\"vocab_size\"])\n",
    "\n",
    "    snip_data = np.load(snippets_npz_path, allow_pickle=True)\n",
    "    intervals = snip_data[\"intervals\"]        # (N, L)\n",
    "    snippet_length = intervals.shape[1]\n",
    "\n",
    "    genres = snip_data.get(\"genres\", None)\n",
    "    labels = snip_data.get(\"snippet_labels\", None)\n",
    "    start_secs = snip_data.get(\"snippet_start_secs\", None)\n",
    "    end_secs   = snip_data.get(\"snippet_end_secs\", None)\n",
    "\n",
    "    # sanity\n",
    "    assert embeddings.shape[0] == intervals.shape[0] == song_ids.shape[0], \\\n",
    "        \"Embeddings, intervals, and song_ids must have same length.\"\n",
    "\n",
    "    return {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"song_ids\": song_ids,\n",
    "        \"midi_filenames\": midi_filenames,\n",
    "        \"min_interval\": min_interval,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"genres\": genres,\n",
    "        \"labels\": labels,\n",
    "        \"start_secs\": start_secs,\n",
    "        \"end_secs\": end_secs,\n",
    "        \"snippet_length\": snippet_length,\n",
    "    }\n",
    "\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    a: (N, D)\n",
    "    b: (D,)\n",
    "    returns: (N,) cosine similarities\n",
    "    \"\"\"\n",
    "    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = b / np.linalg.norm(b)\n",
    "    return a_norm @ b_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66066898-1dd8-48a0-a323-25cc39ea152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_retrieval_by_snippet_index(\n",
    "    query_idx: int,\n",
    "    top_k: int = 10,\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,   # or CONTRASTIVE_EMBED_PATH\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple demo: pick an existing snippet by index and retrieve nearest neighbors\n",
    "    using the precomputed embeddings. No model needed.\n",
    "    \"\"\"\n",
    "    index = load_embedding_index(embed_npz_path, snippets_npz_path)\n",
    "\n",
    "    embeddings = index[\"embeddings\"]\n",
    "    song_ids   = index[\"song_ids\"]\n",
    "    midi_files = index[\"midi_filenames\"]\n",
    "    genres     = index[\"genres\"]\n",
    "    labels     = index[\"labels\"]\n",
    "    start_secs = index[\"start_secs\"]\n",
    "    end_secs   = index[\"end_secs\"]\n",
    "\n",
    "    N = embeddings.shape[0]\n",
    "    if query_idx < 0 or query_idx >= N:\n",
    "        raise ValueError(f\"query_idx {query_idx} out of range [0, {N-1}]\")\n",
    "\n",
    "    sims = cosine_sim(embeddings, embeddings[query_idx])\n",
    "    sims[query_idx] = -np.inf  # exclude self\n",
    "    sorted_idx = np.argsort(-sims)\n",
    "\n",
    "    print(f\"\\n=== Retrieval demo (existing snippet) ===\")\n",
    "    print(f\"Query snippet index: {query_idx}\")\n",
    "    sid_q = int(song_ids[query_idx])\n",
    "    fname_q = midi_files[sid_q] if midi_files is not None else \"N/A\"\n",
    "    genre_q = genres[query_idx] if genres is not None else \"unknown\"\n",
    "    label_q = labels[query_idx] if labels is not None else f\"snippet_{query_idx}\"\n",
    "\n",
    "    print(f\"  query song_id={sid_q}, genre={genre_q}, file={fname_q}\")\n",
    "    print(f\"  label={label_q}\")\n",
    "    if start_secs is not None and end_secs is not None:\n",
    "        print(f\"  approx time: {start_secs[query_idx]:.2f}s → {end_secs[query_idx]:.2f}s\")\n",
    "\n",
    "    print(f\"\\nTop {top_k} neighbors:\")\n",
    "    for rank, idx in enumerate(sorted_idx[:top_k], start=1):\n",
    "        sid = int(song_ids[idx])\n",
    "        fname = midi_files[sid] if midi_files is not None else \"N/A\"\n",
    "        genre = genres[idx] if genres is not None else \"unknown\"\n",
    "        label = labels[idx] if labels is not None else f\"snippet_{idx}\"\n",
    "        print(f\"#{rank:02d}  sim={sims[idx]:.3f}\")\n",
    "        print(f\"     idx={idx}, song_id={sid}, genre={genre}, file={fname}\")\n",
    "        print(f\"     label={label}\")\n",
    "        if start_secs is not None and end_secs is not None:\n",
    "            print(f\"     approx time: {start_secs[idx]:.2f}s → {end_secs[idx]:.2f}s\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45642770-8f0f-404d-8c1b-03cbc1e8f2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieval demo (existing snippet) ===\n",
      "Query snippet index: 201\n",
      "  query song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "  label=classic_Axel_F_1_idx000000_to000032_t0000.00s_to0027.26s\n",
      "  approx time: 0.00s → 27.26s\n",
      "\n",
      "Top 10 neighbors:\n",
      "#01  sim=1.000\n",
      "     idx=204, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000048_to000080_t0040.90s_to0068.16s\n",
      "     approx time: 40.90s → 68.16s\n",
      "\n",
      "#02  sim=0.869\n",
      "     idx=28179, song_id=749, genre=rnb, file=Feel_So_High.mid\n",
      "     label=rnb_Feel_So_High_idx000192_to000224_t0088.79s_to0104.59s\n",
      "     approx time: 88.79s → 104.59s\n",
      "\n",
      "#03  sim=0.835\n",
      "     idx=26177, song_id=704, genre=pop, file=When the Going Gets Tough.mid\n",
      "     label=pop_When the Going Gets Tough_idx000016_to000048_t0008.73s_to0028.51s\n",
      "     approx time: 8.73s → 28.51s\n",
      "\n",
      "#04  sim=0.834\n",
      "     idx=207, song_id=4, genre=classic, file=Axel_F_1.mid\n",
      "     label=classic_Axel_F_1_idx000096_to000128_t0081.79s_to0121.64s\n",
      "     approx time: 81.79s → 121.64s\n",
      "\n",
      "#05  sim=0.824\n",
      "     idx=2484, song_id=63, genre=classic, file=addictedtolove.mid\n",
      "     label=classic_addictedtolove_idx000240_to000272_t0112.05s_to0125.08s\n",
      "     approx time: 112.05s → 125.08s\n",
      "\n",
      "#06  sim=0.821\n",
      "     idx=25238, song_id=678, genre=pop, file=Outstanding.mid\n",
      "     label=pop_Outstanding_idx000400_to000432_t0248.22s_to0268.82s\n",
      "     approx time: 248.22s → 268.82s\n",
      "\n",
      "#07  sim=0.819\n",
      "     idx=12834, song_id=352, genre=folk, file=letyourloveflow.mid\n",
      "     label=folk_letyourloveflow_idx000128_to000160_t0090.52s_to0114.66s\n",
      "     approx time: 90.52s → 114.66s\n",
      "\n",
      "#08  sim=0.818\n",
      "     idx=25213, song_id=678, genre=pop, file=Outstanding.mid\n",
      "     label=pop_Outstanding_idx000000_to000032_t0000.00s_to0012.69s\n",
      "     approx time: 0.00s → 12.69s\n",
      "\n",
      "#09  sim=0.815\n",
      "     idx=16467, song_id=449, genre=guitar, file=new_york_state_of_mind_dwb.mid\n",
      "     label=guitar_new_york_state_of_mind_dwb_idx000080_to000112_t0069.29s_to0100.98s\n",
      "     approx time: 69.29s → 100.98s\n",
      "\n",
      "#10  sim=0.812\n",
      "     idx=28734, song_id=762, genre=rnb, file=Im Your Angel.mid\n",
      "     label=rnb_Im Your Angel_idx000256_to000288_t0207.16s_to0237.24s\n",
      "     approx time: 207.16s → 237.24s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_retrieval_by_snippet_index(\n",
    "    query_idx=201,\n",
    "    top_k=10,\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,  # or CONTRASTIVE_EMBED_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "911d494c-a6e7-420c-bb11-168e3a46f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "from typing import List, Tuple\n",
    "from music21 import converter, instrument, note, chord, stream, key, interval, pitch, tempo\n",
    "\n",
    "# how many notes per snippet (you can tweak)\n",
    "SNIPPET_LENGTH = 32\n",
    "\n",
    "# base rhythmic unit: 1 quarter note = 4 steps, so 1 step = sixteenth note\n",
    "STEPS_PER_QUARTER = 4\n",
    "\n",
    "\n",
    "def load_midi(filepath: Path) -> stream.Score:\n",
    "    \"\"\"Load a MIDI file into a music21 Score.\"\"\"\n",
    "    return converter.parse(str(filepath))\n",
    "\n",
    "\n",
    "def pick_melody_part(score: stream.Score) -> stream.Part | None:\n",
    "    \"\"\"\n",
    "    Heuristic for picking the 'melody' part:\n",
    "\n",
    "    1. Skip *purely* percussion parts.\n",
    "    2. If any part name/instrument name suggests 'melody/lead/right hand',\n",
    "       pick that directly.\n",
    "    3. Otherwise:\n",
    "       - For each remaining part, compute:\n",
    "         * n_notes\n",
    "         * avg_pitch\n",
    "       - Compute median avg_pitch across candidates.\n",
    "       - Filter to parts with avg_pitch >= median (favor higher voices).\n",
    "       - Among those, pick the one with the most notes; break ties by higher avg_pitch.\n",
    "\n",
    "    Returns the chosen Part, or None if nothing suitable is found.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    for p in score.parts:\n",
    "        insts = list(p.getInstruments())\n",
    "\n",
    "        # Determine if this part is purely percussion (all instruments percussion-like)\n",
    "        has_percussion = any(\n",
    "            isinstance(i, instrument.UnpitchedPercussion) or\n",
    "            (\"percussion\" in (i.bestName() or \"\").lower())\n",
    "            for i in insts\n",
    "        )\n",
    "        has_non_percussion = any(\n",
    "            not isinstance(i, instrument.UnpitchedPercussion) and\n",
    "            \"percussion\" not in (i.bestName() or \"\").lower()\n",
    "            for i in insts\n",
    "        )\n",
    "\n",
    "        # Skip only if it's *purely* percussion, not mixed\n",
    "        if has_percussion and not has_non_percussion:\n",
    "            continue\n",
    "\n",
    "        # Collect notes/chords\n",
    "        notes_chords = [n for n in p.recurse().notes if isinstance(n, (note.Note, chord.Chord))]\n",
    "        if not notes_chords:\n",
    "            continue\n",
    "\n",
    "        # Basic stats\n",
    "        pitches = []\n",
    "        for n in notes_chords:\n",
    "            if isinstance(n, note.Note):\n",
    "                pitches.append(n.pitch.midi)\n",
    "            elif isinstance(n, chord.Chord):\n",
    "                pitches.append(max(nn.pitch.midi for nn in n.notes))\n",
    "\n",
    "        if not pitches:\n",
    "            continue\n",
    "\n",
    "        n_notes = len(pitches)\n",
    "        avg_pitch = sum(pitches) / len(pitches)\n",
    "\n",
    "        # part/instrument names (lowercased)\n",
    "        part_name = (p.partName or \"\").lower()\n",
    "        inst_names = [str(inst.instrumentName or \"\").lower()\n",
    "                      for inst in insts]\n",
    "\n",
    "        candidates.append({\n",
    "            \"part\": p,\n",
    "            \"n_notes\": n_notes,\n",
    "            \"avg_pitch\": avg_pitch,\n",
    "            \"part_name\": part_name,\n",
    "            \"inst_names\": inst_names,\n",
    "        })\n",
    "\n",
    "    if not candidates:\n",
    "        print(\"  [warn] no suitable melodic parts; skipping this file.\")\n",
    "        return None\n",
    "\n",
    "    # 1) Name-based shortcut: if any part name/instrument suggests \"melody\"\n",
    "    name_keywords = [\n",
    "        \"melody\", \"lead\", \"right hand\", \"rh\", \"treble\", \"solo\", \"violin\", \"flute\", \"trumpet\", \"saw wave\"\n",
    "    ]\n",
    "\n",
    "    def looks_like_melody(c):\n",
    "        text = c[\"part_name\"] + \" \" + \" \".join(c[\"inst_names\"])\n",
    "        text = text.lower()\n",
    "        return any(kw in text for kw in name_keywords)\n",
    "\n",
    "    name_candidates = [c for c in candidates if looks_like_melody(c)]\n",
    "    if name_candidates:\n",
    "        # among these, pick the one with highest avg_pitch (just in case)\n",
    "        best = max(name_candidates, key=lambda c: c[\"avg_pitch\"])\n",
    "        print(f\"  [info] pick_melody_part: selected by name heuristic: \"\n",
    "              f\"part_name='{best['part_name']}', avg_pitch={best['avg_pitch']:.1f}, n_notes={best['n_notes']}\")\n",
    "        return best[\"part\"]\n",
    "\n",
    "    # 2) Pitch-based filtering: keep only parts at or above median avg_pitch\n",
    "    avg_pitches = [c[\"avg_pitch\"] for c in candidates]\n",
    "    median_pitch = sorted(avg_pitches)[len(avg_pitches) // 2]\n",
    "\n",
    "    high_voice_candidates = [c for c in candidates if c[\"avg_pitch\"] >= median_pitch]\n",
    "    if not high_voice_candidates:\n",
    "        high_voice_candidates = candidates  # fallback to all\n",
    "\n",
    "    # 3) Among high-voice candidates, pick the one with the most notes & higher pitch\n",
    "    best = max(\n",
    "        high_voice_candidates,\n",
    "        key=lambda c: (c[\"n_notes\"], c[\"avg_pitch\"])  # primary: many notes, secondary: higher pitch\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  [info] pick_melody_part: selected by stats: \"\n",
    "        f\"part_name='{best['part_name']}', avg_pitch={best['avg_pitch']:.1f}, \"\n",
    "        f\"n_notes={best['n_notes']}\"\n",
    "    )\n",
    "\n",
    "    return best[\"part\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_key_and_transpose(melody: stream.Part) -> stream.Part:\n",
    "    \"\"\"\n",
    "    Detect key with music21 and transpose so tonic is C (for major) or A (for minor).\n",
    "    If key detection fails for some reason, return the original melody.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        key_guess = melody.analyze('key')\n",
    "    except Exception as e:\n",
    "        print(\"  [warn] key analysis failed, leaving melody untransposed:\", e)\n",
    "        return melody\n",
    "\n",
    "    # Decide target tonic\n",
    "    if key_guess.mode == 'major':\n",
    "        target_pitch = pitch.Pitch('C')\n",
    "    else:\n",
    "        # treat minor keys as aiming for A minor tonic\n",
    "        target_pitch = pitch.Pitch('A')\n",
    "\n",
    "    # Build interval from current tonic to target tonic\n",
    "    itvl = interval.Interval(key_guess.tonic, target_pitch)\n",
    "\n",
    "    transposed = melody.transpose(itvl)\n",
    "    return transposed\n",
    "\n",
    "\n",
    "def extract_pitch_duration_sequence(melody: stream.Part) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Extract (midi_pitch, quarter_length_duration) from a melody line.\n",
    "    Ignore rests; collapse chords to their top note.\n",
    "    \"\"\"\n",
    "    seq = []\n",
    "    for elem in melody.recurse().notesAndRests:\n",
    "        if isinstance(elem, note.Note):\n",
    "            midi_pitch = elem.pitch.midi\n",
    "            dur = float(elem.quarterLength)\n",
    "            seq.append((midi_pitch, dur))\n",
    "        elif isinstance(elem, chord.Chord):\n",
    "            # take highest note in chord as melody approximation\n",
    "            midi_pitch = max(n.pitch.midi for n in elem.notes)\n",
    "            dur = float(elem.quarterLength)\n",
    "            seq.append((midi_pitch, dur))\n",
    "        else:\n",
    "            # ignore rests and other stuff for now\n",
    "            continue\n",
    "    return seq\n",
    "\n",
    "\n",
    "def convert_to_intervals_and_durations(\n",
    "    pitch_dur_seq: List[Tuple[int, float]]\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Convert absolute pitches to pitch intervals and durations to integer steps.\n",
    "    intervals[i] = pitch[i] - pitch[i-1], with first interval = 0\n",
    "    durations[i] = round( quarter_length * STEPS_PER_QUARTER )\n",
    "    \"\"\"\n",
    "    if not pitch_dur_seq:\n",
    "        return [], []\n",
    "\n",
    "    pitches = [p for (p, _) in pitch_dur_seq]\n",
    "    durs_q = [d for (_, d) in pitch_dur_seq]\n",
    "\n",
    "    intervals = [0]  # first note has no previous reference\n",
    "    for i in range(1, len(pitches)):\n",
    "        intervals.append(int(pitches[i] - pitches[i - 1]))\n",
    "\n",
    "    durations = [max(1, int(round(d * STEPS_PER_QUARTER))) for d in durs_q]\n",
    "\n",
    "    return intervals, durations\n",
    "\n",
    "\n",
    "def make_snippets(\n",
    "    intervals: List[int],\n",
    "    durations: List[int],\n",
    "    snippet_length: int = SNIPPET_LENGTH\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Slice sequences into fixed-length snippets.\n",
    "    We use a simple sliding window with stride = snippet_length // 2 (50% overlap).\n",
    "    Short sequences yield zero snippets.\n",
    "    \"\"\"\n",
    "    assert len(intervals) == len(durations)\n",
    "    n = len(intervals)\n",
    "    if n < snippet_length:\n",
    "        return np.empty((0, snippet_length), dtype=np.int32), np.empty((0, snippet_length), dtype=np.int32)\n",
    "\n",
    "    stride = snippet_length // 2\n",
    "    interval_snips = []\n",
    "    duration_snips = []\n",
    "\n",
    "    for start in range(0, n - snippet_length + 1, stride):\n",
    "        end = start + snippet_length\n",
    "        interval_snips.append(intervals[start:end])\n",
    "        duration_snips.append(durations[start:end])\n",
    "\n",
    "    return np.array(interval_snips, dtype=np.int32), np.array(duration_snips, dtype=np.int32)\n",
    "\n",
    "def make_snippets_with_timestamps(\n",
    "    intervals: List[int],\n",
    "    durations: List[int],\n",
    "    durations_q: List[float],\n",
    "    snippet_length: int = SNIPPET_LENGTH\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Slice sequences into fixed-length snippets.\n",
    "    Also return timestamp pairs (start_q, end_q) in quarter lengths.\n",
    "    \"\"\"\n",
    "    assert len(intervals) == len(durations)\n",
    "    n = len(intervals)\n",
    "    if n < snippet_length:\n",
    "        return np.empty((0, snippet_length), dtype=np.int32), np.empty((0, snippet_length), dtype=np.int32), []\n",
    "\n",
    "    stride = snippet_length // 2\n",
    "    interval_snips, duration_snips, timestamps = [], [], []\n",
    "\n",
    "    cumulative_q = np.cumsum([0] + durations_q)  # cumulative time in quarter lengths\n",
    "\n",
    "    for start in range(0, n - snippet_length + 1, stride):\n",
    "        end = start + snippet_length\n",
    "        interval_snips.append(intervals[start:end])\n",
    "        duration_snips.append(durations[start:end])\n",
    "\n",
    "        start_q = cumulative_q[start]\n",
    "        end_q = cumulative_q[end]\n",
    "        timestamps.append((int(round(start_q)), int(round(end_q))))\n",
    "\n",
    "    return np.array(interval_snips, dtype=np.int32), np.array(duration_snips, dtype=np.int32), timestamps\n",
    "\n",
    "from music21 import instrument\n",
    "\n",
    "def sanitize_melody_instrument(melody_part):\n",
    "    \"\"\"\n",
    "    Remove any existing Instrument metadata and force a clean, non-percussion\n",
    "    instrument on a non-drum channel.\n",
    "    \"\"\"\n",
    "    # Remove ALL Instrument objects from this part\n",
    "    for inst in list(melody_part.recurse().getElementsByClass(instrument.Instrument)):\n",
    "        try:\n",
    "            melody_part.remove(inst)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Set a friendly part name\n",
    "    melody_part.partName = \"Melody\"\n",
    "\n",
    "    # Insert one clean Piano instrument at the beginning\n",
    "    piano = instrument.Piano()\n",
    "    piano.midiProgram = 0  # Acoustic Grand\n",
    "    piano.midiChannel = 0  # Channel 1 (NOT 10/drums)\n",
    "    melody_part.insert(0, piano)\n",
    "\n",
    "    return melody_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8971e3c7-f129-4642-bdef-cc616efe01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from music21 import stream, note, pitch\n",
    "\n",
    "STEPS_PER_QUARTER = 4  # must match your preprocessing\n",
    "\n",
    "# Where to save snippet MIDIs\n",
    "SNIPPET_MIDI_DIR = Path(\"../data/demo/snippet_midis\")\n",
    "QUERY_SNIPPET_MIDI_DIR = SNIPPET_MIDI_DIR / \"query\"\n",
    "LIB_SNIPPET_MIDI_DIR = SNIPPET_MIDI_DIR / \"retrieved\"\n",
    "\n",
    "for d in [SNIPPET_MIDI_DIR, QUERY_SNIPPET_MIDI_DIR, LIB_SNIPPET_MIDI_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def snippet_to_stream(interval_seq, duration_seq, base_midi_pitch=60):\n",
    "    \"\"\"\n",
    "    Convert one snippet (intervals, durations in *steps*) into a music21 Stream.\n",
    "    base_midi_pitch: starting pitch (60 = middle C).\n",
    "    \"\"\"\n",
    "    s = stream.Stream()\n",
    "    current_pitch = base_midi_pitch\n",
    "    \n",
    "    for interval_val, dur_steps in zip(interval_seq, duration_seq):\n",
    "        current_pitch += int(interval_val)\n",
    "        p = pitch.Pitch()\n",
    "        p.midi = current_pitch\n",
    "        \n",
    "        ql = float(dur_steps) / STEPS_PER_QUARTER  # back to quarterLength\n",
    "        \n",
    "        n = note.Note(p)\n",
    "        n.quarterLength = ql\n",
    "        s.append(n)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def save_library_snippet_as_midi(\n",
    "    lib_idx: int,\n",
    "    intervals_arr: np.ndarray,\n",
    "    durations_arr: np.ndarray,\n",
    "    song_ids: np.ndarray,\n",
    "    out_dir: Path = LIB_SNIPPET_MIDI_DIR,\n",
    "    base_midi_pitch: int = 60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save a library snippet (by global index) to MIDI.\n",
    "    \"\"\"\n",
    "    if lib_idx < 0 or lib_idx >= intervals_arr.shape[0]:\n",
    "        raise ValueError(f\"lib_idx {lib_idx} out of range [0, {intervals_arr.shape[0]-1}]\")\n",
    "    \n",
    "    interval_seq = intervals_arr[lib_idx]\n",
    "    duration_seq = durations_arr[lib_idx]\n",
    "    \n",
    "    s = snippet_to_stream(interval_seq, duration_seq, base_midi_pitch=base_midi_pitch)\n",
    "    \n",
    "    out_path = out_dir / f\"lib_snippet_{lib_idx}_song{song_ids[lib_idx]}.mid\"\n",
    "    s.write(\"midi\", fp=str(out_path))\n",
    "    print(f\"Saved library snippet {lib_idx} (song_id={song_ids[lib_idx]}) to {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def save_query_snippet_as_midi(\n",
    "    q_idx: int,\n",
    "    query_intervals: np.ndarray,\n",
    "    query_durations: np.ndarray,\n",
    "    out_dir: Path = QUERY_SNIPPET_MIDI_DIR,\n",
    "    base_midi_pitch: int = 60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save a query (input-song) snippet (by local index) to MIDI.\n",
    "    query_intervals: (Q, L)\n",
    "    query_durations: (Q, L)\n",
    "    \"\"\"\n",
    "    if q_idx < 0 or q_idx >= query_intervals.shape[0]:\n",
    "        raise ValueError(f\"q_idx {q_idx} out of range [0, {query_intervals.shape[0]-1}]\")\n",
    "    \n",
    "    interval_seq = query_intervals[q_idx]\n",
    "    duration_seq = query_durations[q_idx]\n",
    "    \n",
    "    s = snippet_to_stream(interval_seq, duration_seq, base_midi_pitch=base_midi_pitch)\n",
    "    \n",
    "    out_path = out_dir / f\"query_snippet_{q_idx}.mid\"\n",
    "    s.write(\"midi\", fp=str(out_path))\n",
    "    print(f\"Saved query snippet {q_idx} to {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48843c9a-dad2-445b-9539-651ab7b5e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import tempo\n",
    "\n",
    "def embed_midi_file_to_snippets(\n",
    "    midi_path,\n",
    "    model,\n",
    "    min_interval: int,\n",
    "    snippet_length: int,\n",
    "    device=None,\n",
    "    model_type: str = \"auto\",  # \"auto\" or \"contrastive\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        embeddings: (Q, D)\n",
    "        meta: dict with:\n",
    "          - start_indices, end_indices\n",
    "          - start_qs, end_qs\n",
    "          - start_secs, end_secs\n",
    "          - interval_snips: (Q, L)\n",
    "          - duration_snips: (Q, L)  [in STEPS, like training]\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    midi_path = Path(midi_path)\n",
    "    print(f\"Loading query MIDI: {midi_path.name}\")\n",
    "    score = load_midi(midi_path)\n",
    "\n",
    "    melody = pick_melody_part(score)\n",
    "    if melody is None:\n",
    "        print(\"  [warn] No melodic part found; returning empty.\")\n",
    "        return np.empty((0, 128)), {}\n",
    "\n",
    "    # transpose + sanitize (same as training preprocessing)\n",
    "    melody_transposed = detect_key_and_transpose(melody)\n",
    "    melody_transposed = sanitize_melody_instrument(melody_transposed)\n",
    "\n",
    "    # extract pitch/duration seq\n",
    "    pitch_dur_seq = extract_pitch_duration_sequence(melody_transposed)\n",
    "    print(f\"  [info] extracted {len(pitch_dur_seq)} melodic events.\")\n",
    "\n",
    "    if len(pitch_dur_seq) < snippet_length:\n",
    "        print(f\"  [warn] Too few notes (< {snippet_length}) → no snippets.\")\n",
    "        return np.empty((0, 128)), {}\n",
    "\n",
    "    # absolute pitches & quarter-length durations\n",
    "    durs_q = [d for (p, d) in pitch_dur_seq]\n",
    "\n",
    "    # convert to intervals + quantized duration steps\n",
    "    intervals, durations_steps = convert_to_intervals_and_durations(pitch_dur_seq)\n",
    "\n",
    "    # ---- Slice into snippets with indices & quarter-time ----\n",
    "    N = len(intervals)\n",
    "    stride = snippet_length // 2\n",
    "\n",
    "    cum_q = np.cumsum([0.0] + list(durs_q))  # len N+1\n",
    "\n",
    "    all_interval_snips = []\n",
    "    all_duration_snips = []\n",
    "    all_start_idx = []\n",
    "    all_end_idx = []\n",
    "    all_start_q = []\n",
    "    all_end_q = []\n",
    "\n",
    "    for start in range(0, N - snippet_length + 1, stride):\n",
    "        end = start + snippet_length\n",
    "        all_interval_snips.append(intervals[start:end])\n",
    "        all_duration_snips.append(durations_steps[start:end])\n",
    "        all_start_idx.append(start)\n",
    "        all_end_idx.append(end)\n",
    "        all_start_q.append(cum_q[start])\n",
    "        all_end_q.append(cum_q[end])\n",
    "\n",
    "    interval_snips = np.array(all_interval_snips, dtype=np.int32)     # (Q, L)\n",
    "    duration_snips = np.array(all_duration_snips, dtype=np.int32)     # (Q, L)\n",
    "    start_indices = np.array(all_start_idx, dtype=np.int32)\n",
    "    end_indices = np.array(all_end_idx, dtype=np.int32)\n",
    "    start_qs = np.array(all_start_q, dtype=float)\n",
    "    end_qs = np.array(all_end_q, dtype=float)\n",
    "\n",
    "    num_snips = interval_snips.shape[0]\n",
    "    print(f\"  [info] created {num_snips} snippets of length {snippet_length}.\")\n",
    "\n",
    "    # ---- Approximate seconds using tempo ----\n",
    "    tempos = score.flatten().getElementsByClass(tempo.MetronomeMark)\n",
    "    if tempos:\n",
    "        bpm = tempos[0].number or 120.0\n",
    "    else:\n",
    "        bpm = 120.0\n",
    "    sec_per_q = 60.0 / bpm\n",
    "    start_secs = start_qs * sec_per_q\n",
    "    end_secs = end_qs * sec_per_q\n",
    "\n",
    "    # ---- Map intervals to token IDs (shift by min_interval) ----\n",
    "    shifted_snips = interval_snips - min_interval  # (Q, L)\n",
    "    shifted_snips = np.clip(shifted_snips, 0, None)  # just in case\n",
    "\n",
    "    x = torch.tensor(shifted_snips, dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if model_type == \"auto\":\n",
    "            z = model.encode(x)    # (Q, H)\n",
    "        else:  # contrastive encoder directly returns normalized embedding\n",
    "            z = model(x)           # (Q, D)\n",
    "\n",
    "    embeddings = z.cpu().numpy()\n",
    "\n",
    "    meta = {\n",
    "        \"start_indices\": start_indices,\n",
    "        \"end_indices\": end_indices,\n",
    "        \"start_qs\": start_qs,\n",
    "        \"end_qs\": end_qs,\n",
    "        \"start_secs\": start_secs,\n",
    "        \"end_secs\": end_secs,\n",
    "        \"interval_snips\": interval_snips,\n",
    "        \"duration_snips\": duration_snips,\n",
    "    }\n",
    "\n",
    "    return embeddings, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a4861c7-e652-4596-a7f1-7d14c05f501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_retrieval_for_midi_snippets(\n",
    "    midi_path,\n",
    "    model,\n",
    "    model_type: str = \"auto\",          # \"auto\" or \"contrastive\"\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    "    top_k: int = 10,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a given MIDI file:\n",
    "\n",
    "      1) Slice into snippets + embed each snippet.\n",
    "      2) Compute cosine similarity between *every* query snippet and *every* library snippet.\n",
    "      3) For each library snippet, take the maximum similarity over all query snippets.\n",
    "      4) Show top_k library snippets.\n",
    "      5) Additionally: save the BEST query snippet and BEST library snippet as MIDI.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load library embeddings + metadata ---\n",
    "    index = load_embedding_index(embed_npz_path, snippets_npz_path)\n",
    "    embeddings   = index[\"embeddings\"]       # (N, D)\n",
    "    song_ids_emb = index[\"song_ids\"]         # (N,)\n",
    "    midi_files   = index[\"midi_filenames\"]   # per-song\n",
    "    genres       = index[\"genres\"]\n",
    "    labels       = index[\"labels\"]\n",
    "    start_secs   = index[\"start_secs\"]\n",
    "    end_secs     = index[\"end_secs\"]\n",
    "    min_interval = index[\"min_interval\"]\n",
    "    snippet_length = index[\"snippet_length\"]\n",
    "\n",
    "    # Also need intervals & durations for library snippets to reconstruct MIDI\n",
    "    snip_data = np.load(snippets_npz_path, allow_pickle=True)\n",
    "    intervals_arr = snip_data[\"intervals\"]     # (N, L)\n",
    "    durations_arr = snip_data[\"durations\"]     # (N, L)\n",
    "    song_ids_snip = snip_data[\"song_ids\"]      # (N,)\n",
    "    # sanity check: should match\n",
    "    assert np.array_equal(song_ids_emb, song_ids_snip)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Embed the query MIDI into snippet embeddings ---\n",
    "    query_embs, meta = embed_midi_file_to_snippets(\n",
    "        midi_path=midi_path,\n",
    "        model=model,\n",
    "        min_interval=min_interval,\n",
    "        snippet_length=snippet_length,\n",
    "        device=device,\n",
    "        model_type=model_type,\n",
    "    )\n",
    "\n",
    "    Q = query_embs.shape[0]\n",
    "    if Q == 0:\n",
    "        print(\"No query snippets to retrieve with.\")\n",
    "        return []\n",
    "\n",
    "    query_interval_snips = meta[\"interval_snips\"]   # (Q, L)\n",
    "    query_duration_snips = meta[\"duration_snips\"]   # (Q, L)\n",
    "\n",
    "    # Normalize library and query embeddings\n",
    "    lib_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)  # (N, D)\n",
    "    q_norm   = query_embs / np.linalg.norm(query_embs, axis=1, keepdims=True)  # (Q, D)\n",
    "\n",
    "    # --- Similarity matrix: query_snippets x library_snippets ---\n",
    "    sim_matrix = q_norm @ lib_norm.T  # (Q, N)\n",
    "\n",
    "    # For each library snippet, take the best similarity over all query snippets\n",
    "    best_sim_per_lib = sim_matrix.max(axis=0)      # (N,)\n",
    "    best_q_idx_per_lib = sim_matrix.argmax(axis=0) # (N,)\n",
    "\n",
    "    # Sort library snippets by descending similarity\n",
    "    sorted_lib_idx = np.argsort(-best_sim_per_lib)\n",
    "\n",
    "    top_k = min(top_k, embeddings.shape[0])\n",
    "\n",
    "    print(f\"\\n=== Global snippet-level retrieval for: {Path(midi_path).name} ===\")\n",
    "    print(f\"Total query snippets from this song: {Q}\")\n",
    "    print(f\"Showing top {top_k} library snippets (across ALL query snippets).\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for rank, lib_idx in enumerate(sorted_lib_idx[:top_k], start=1):\n",
    "        sim_score = best_sim_per_lib[lib_idx]\n",
    "        qid = best_q_idx_per_lib[lib_idx]  # which query snippet gave this best match\n",
    "\n",
    "        # query snippet timing\n",
    "        q_s_sec = meta[\"start_secs\"][qid]\n",
    "        q_e_sec = meta[\"end_secs\"][qid]\n",
    "\n",
    "        # library snippet metadata\n",
    "        sid   = int(song_ids_emb[lib_idx])\n",
    "        fname = midi_files[sid] if midi_files is not None else \"N/A\"\n",
    "        genre = genres[lib_idx] if genres is not None else \"unknown\"\n",
    "        label = labels[lib_idx] if labels is not None else f\"snippet_{lib_idx}\"\n",
    "        s_sec = start_secs[lib_idx] if start_secs is not None else None\n",
    "        e_sec = end_secs[lib_idx] if end_secs is not None else None\n",
    "\n",
    "        print(f\"#{rank:02d}  sim={sim_score:.3f}\")\n",
    "        print(f\"     BEST query snippet #{qid} in input song\")\n",
    "        print(f\"       query time:  {q_s_sec:7.2f}s → {q_e_sec:7.2f}s\")\n",
    "        print(f\"     library idx={lib_idx}, song_id={sid}, genre={genre}, file={fname}\")\n",
    "        print(f\"       label: {label}\")\n",
    "        if s_sec is not None and e_sec is not None:\n",
    "            print(f\"       library time: {s_sec:7.2f}s → {e_sec:7.2f}s\")\n",
    "        print()\n",
    "\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"similarity\": float(sim_score),\n",
    "            \"library_idx\": int(lib_idx),\n",
    "            \"library_song_id\": sid,\n",
    "            \"library_file\": fname,\n",
    "            \"library_genre\": genre,\n",
    "            \"library_label\": label,\n",
    "            \"library_start_sec\": float(s_sec) if s_sec is not None else None,\n",
    "            \"library_end_sec\": float(e_sec) if e_sec is not None else None,\n",
    "            \"query_snippet_idx\": int(qid),\n",
    "            \"query_start_sec\": float(q_s_sec),\n",
    "            \"query_end_sec\": float(q_e_sec),\n",
    "        })\n",
    "\n",
    "    # ---- Save the BEST matching pair as MIDI ----\n",
    "    if len(sorted_lib_idx) > 0:\n",
    "        best_lib_idx = sorted_lib_idx[0]\n",
    "        best_q_idx = best_q_idx_per_lib[best_lib_idx]\n",
    "\n",
    "        print(\"Saving best matching query and library snippets as MIDI...\")\n",
    "\n",
    "        # Query snippet MIDI\n",
    "        q_midi_path = save_query_snippet_as_midi(\n",
    "            q_idx=best_q_idx,\n",
    "            query_intervals=query_interval_snips,\n",
    "            query_durations=query_duration_snips,\n",
    "            out_dir=QUERY_SNIPPET_MIDI_DIR,\n",
    "            base_midi_pitch=60,\n",
    "        )\n",
    "\n",
    "        # Library snippet MIDI\n",
    "        lib_midi_path = save_library_snippet_as_midi(\n",
    "            lib_idx=best_lib_idx,\n",
    "            intervals_arr=intervals_arr,\n",
    "            durations_arr=durations_arr,\n",
    "            song_ids=song_ids_emb,\n",
    "            out_dir=LIB_SNIPPET_MIDI_DIR,\n",
    "            base_midi_pitch=60,\n",
    "        )\n",
    "\n",
    "        print(f\"Best query snippet saved to:    {q_midi_path}\")\n",
    "        print(f\"Best library snippet saved to:  {lib_midi_path}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "081c1be1-51c9-4eb1-8e4a-5263601af8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# ---------------------\n",
    "# Autoencoder model\n",
    "# ---------------------\n",
    "\n",
    "class MelodyAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder_rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder_rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.output_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Learned start token for the decoder\n",
    "        self.start_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def encode(self, x):\n",
    "        emb = self.embed(x)\n",
    "        _, h_n = self.encoder_rnn(emb)\n",
    "        return h_n[-1]  # (B, H)\n",
    "\n",
    "    def decode(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        z: (B, H)\n",
    "        seq_len: int (L)\n",
    "        Decoder only gets z + a learned start vector, not the target tokens.\n",
    "        \"\"\"\n",
    "        B = z.size(0)\n",
    "        h0 = z.unsqueeze(0)              # (1, B, H)\n",
    "\n",
    "        # Repeat a learned start embedding L times as input\n",
    "        # shape: (B, L, E)\n",
    "        start_emb = self.start_token.expand(B, seq_len, -1)\n",
    "\n",
    "        out, _ = self.decoder_rnn(start_emb, h0)  # (B, L, H)\n",
    "        logits = self.output_fc(out)              # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        L = x.shape[1]\n",
    "        logits = self.decode(z, L)\n",
    "        return logits, z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5407939b-6acf-4cc0-95b4-6754e07605bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MelodyEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, proj_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder_rnn = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        # hidden_dim * 2 because bidirectional\n",
    "        self.proj = nn.Linear(hidden_dim * 2, proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L) token ids\n",
    "        returns: (B, D) L2-normalized embedding\n",
    "        \"\"\"\n",
    "        emb = self.token_embed(x)           # (B, L, E)\n",
    "        _, h_n = self.encoder_rnn(emb)      # (2*num_layers, B, H)\n",
    "        h_fw = h_n[-2]                      # (B, H)\n",
    "        h_bw = h_n[-1]                      # (B, H)\n",
    "        h_cat = torch.cat([h_fw, h_bw], dim=-1)  # (B, 2H)\n",
    "        z = self.proj(h_cat)               # (B, D)\n",
    "        z = F.normalize(z, dim=-1)         # L2-normalize for cosine similarity\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30b27a13-354e-443e-823c-bca63140acf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MelodyEncoder(\n",
       "  (token_embed): Embedding(184, 128)\n",
       "  (encoder_rnn): GRU(128, 256, batch_first=True, bidirectional=True)\n",
       "  (proj): Linear(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load min_interval & vocab_size from any embedding NPZ\n",
    "idx_info = load_embedding_index(AUTOENC_EMBED_PATH, SNIPPETS_PATH)\n",
    "vocab_size = idx_info[\"vocab_size\"]\n",
    "\n",
    "# 1) Autoencoder\n",
    "auto_model = MelodyAutoencoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=64,    # SAME values used during training\n",
    "    hidden_dim=128,\n",
    ")\n",
    "\n",
    "auto_model.load_state_dict(torch.load(\"../models/autoencoder.pt\", map_location=device))\n",
    "auto_model.to(device)\n",
    "auto_model.eval()\n",
    "\n",
    "# 2) Contrastive encoder\n",
    "contrastive_model = MelodyEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,    # same as training\n",
    "    hidden_dim=256 ,\n",
    "    proj_dim=128,\n",
    "    num_layers=1,\n",
    ")\n",
    "\n",
    "contrastive_model.load_state_dict(torch.load(\"../models/contrastive_encoder.pt\", map_location=device))\n",
    "contrastive_model.to(device)\n",
    "contrastive_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ec9dbda-b9b8-473f-bf28-3522f6b5ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading query MIDI: MAROON 5.She will be loved K.MID\n",
      "  [info] pick_melody_part: selected by name heuristic: part_name='she will be loved', avg_pitch=63.1, n_notes=2014\n",
      "  [info] extracted 2014 melodic events.\n",
      "  [info] created 124 snippets of length 32.\n",
      "\n",
      "=== Global snippet-level retrieval for: MAROON 5.She will be loved K.MID ===\n",
      "Total query snippets from this song: 124\n",
      "Showing top 10 library snippets (across ALL query snippets).\n",
      "\n",
      "#01  sim=0.866\n",
      "     BEST query snippet #18 in input song\n",
      "       query time:   207.40s →  228.63s\n",
      "     library idx=24031, song_id=655, genre=pop, file=Horny 98.mid\n",
      "       label: pop_Horny 98_idx000336_to000368_t0094.04s_to0102.07s\n",
      "       library time:   94.04s →  102.07s\n",
      "\n",
      "#02  sim=0.834\n",
      "     BEST query snippet #53 in input song\n",
      "       query time:   653.14s →  675.74s\n",
      "     library idx=11233, song_id=308, genre=folk, file=Lodi_2.mid\n",
      "       label: folk_Lodi_2_idx000368_to000400_t0062.31s_to0068.03s\n",
      "       library time:   62.31s →   68.03s\n",
      "\n",
      "#03  sim=0.833\n",
      "     BEST query snippet #86 in input song\n",
      "       query time:  1069.56s → 1088.97s\n",
      "     library idx=11221, song_id=308, genre=folk, file=Lodi_2.mid\n",
      "       label: folk_Lodi_2_idx000176_to000208_t0029.33s_to0035.05s\n",
      "       library time:   29.33s →   35.05s\n",
      "\n",
      "#04  sim=0.830\n",
      "     BEST query snippet #115 in input song\n",
      "       query time:  1447.79s → 1475.74s\n",
      "     library idx=6327, song_id=158, genre=dance, file=Verdammt.mid\n",
      "       label: dance_Verdammt_idx000544_to000576_t0106.67s_to0112.69s\n",
      "       library time:  106.67s →  112.69s\n",
      "\n",
      "#05  sim=0.830\n",
      "     BEST query snippet #32 in input song\n",
      "       query time:   363.53s →  399.51s\n",
      "     library idx=33558, song_id=884, genre=rock, file=WORLD PARTY.Put the message in the box.mid\n",
      "       label: rock_WORLD PARTY.Put the message in the box_idx000176_to000208_t0031.46s_to0038.03s\n",
      "       library time:   31.46s →   38.03s\n",
      "\n",
      "#06  sim=0.830\n",
      "     BEST query snippet #47 in input song\n",
      "       query time:   551.67s →  573.33s\n",
      "     library idx=18689, song_id=511, genre=hip-hop, file=Santana_Into_the_night.mid\n",
      "       label: hip-hop_Santana_Into_the_night_idx001136_to001168_t0175.62s_to0179.46s\n",
      "       library time:  175.62s →  179.46s\n",
      "\n",
      "#07  sim=0.828\n",
      "     BEST query snippet #32 in input song\n",
      "       query time:   363.53s →  399.51s\n",
      "     library idx=20049, song_id=553, genre=piano, file=Beethoven _ Moonlight Sonata Op 27 No 2 Mvt 1.mid\n",
      "       label: piano_Beethoven _ Moonlight Sonata Op 27 No 2 Mvt 1_idx000304_to000336_t0087.52s_to0096.04s\n",
      "       library time:   87.52s →   96.04s\n",
      "\n",
      "#08  sim=0.828\n",
      "     BEST query snippet #19 in input song\n",
      "       query time:   220.05s →  238.48s\n",
      "     library idx=11489, song_id=313, genre=folk, file=No_Myth.mid\n",
      "       label: folk_No_Myth_idx000016_to000048_t0003.49s_to0011.33s\n",
      "       library time:    3.49s →   11.33s\n",
      "\n",
      "#09  sim=0.827\n",
      "     BEST query snippet #87 in input song\n",
      "       query time:  1078.38s → 1115.88s\n",
      "     library idx=11513, song_id=313, genre=folk, file=No_Myth.mid\n",
      "       label: folk_No_Myth_idx000400_to000432_t0111.48s_to0121.71s\n",
      "       library time:  111.48s →  121.71s\n",
      "\n",
      "#10  sim=0.826\n",
      "     BEST query snippet #13 in input song\n",
      "       query time:   146.32s →  167.79s\n",
      "     library idx=10535, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx000816_to000848_t0131.11s_to0135.96s\n",
      "       library time:  131.11s →  135.96s\n",
      "\n",
      "Saving best matching query and library snippets as MIDI...\n",
      "Saved query snippet 18 to ../data/demo/snippet_midis/query/query_snippet_18.mid\n",
      "Saved library snippet 24031 (song_id=655) to ../data/demo/snippet_midis/retrieved/lib_snippet_24031_song655.mid\n",
      "Best query snippet saved to:    ../data/demo/snippet_midis/query/query_snippet_18.mid\n",
      "Best library snippet saved to:  ../data/demo/snippet_midis/retrieved/lib_snippet_24031_song655.mid\n"
     ]
    }
   ],
   "source": [
    "# after training:\n",
    "# result = train_autoencoder(...)\n",
    "# auto_model = result[\"model\"]   # or whatever variable you kept\n",
    "\n",
    "demo_midi_path = \"../data/demo/MAROON 5.She will be loved K.MID\"\n",
    "\n",
    "results = demo_retrieval_for_midi_snippets(\n",
    "    midi_path=demo_midi_path,\n",
    "    model=auto_model,                # or contrastive_model\n",
    "    model_type=\"auto\",               # \"contrastive\" if using contrastive encoder\n",
    "    embed_npz_path=AUTOENC_EMBED_PATH,\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    "    top_k=10,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c3102ed-e9be-4d6d-a92a-3ef9004b7c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading query MIDI: ABBA.Mamma Mia K.mid\n",
      "  [info] pick_melody_part: selected by stats: part_name='mamma mia', avg_pitch=64.2, n_notes=877\n",
      "  [info] extracted 877 melodic events.\n",
      "  [info] created 53 snippets of length 32.\n",
      "\n",
      "=== Global snippet-level retrieval for: ABBA.Mamma Mia K.mid ===\n",
      "Total query snippets from this song: 53\n",
      "Showing top 10 library snippets (across ALL query snippets).\n",
      "\n",
      "#01  sim=0.862\n",
      "     BEST query snippet #41 in input song\n",
      "       query time:   376.95s →  387.52s\n",
      "     library idx=10567, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx001328_to001360_t0212.50s_to0217.13s\n",
      "       library time:  212.50s →  217.13s\n",
      "\n",
      "#02  sim=0.856\n",
      "     BEST query snippet #41 in input song\n",
      "       query time:   376.95s →  387.52s\n",
      "     library idx=10570, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx001376_to001408_t0220.15s_to0225.25s\n",
      "       library time:  220.15s →  225.25s\n",
      "\n",
      "#03  sim=0.854\n",
      "     BEST query snippet #41 in input song\n",
      "       query time:   376.95s →  387.52s\n",
      "     library idx=10569, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx001360_to001392_t0217.13s_to0222.70s\n",
      "       library time:  217.13s →  222.70s\n",
      "\n",
      "#04  sim=0.835\n",
      "     BEST query snippet #41 in input song\n",
      "       query time:   376.95s →  387.52s\n",
      "     library idx=10571, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx001392_to001424_t0222.70s_to0227.79s\n",
      "       library time:  222.70s →  227.79s\n",
      "\n",
      "#05  sim=0.832\n",
      "     BEST query snippet #41 in input song\n",
      "       query time:   376.95s →  387.52s\n",
      "     library idx=10564, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx001280_to001312_t0206.03s_to0209.85s\n",
      "       library time:  206.03s →  209.85s\n",
      "\n",
      "#06  sim=0.829\n",
      "     BEST query snippet #50 in input song\n",
      "       query time:   448.76s →  466.62s\n",
      "     library idx=2322, song_id=61, genre=classic, file=You've_Lost_That_Loving_Feeling_1.mid\n",
      "       label: classic_You've_Lost_That_Loving_Feeling_1_idx000176_to000208_t0056.86s_to0069.04s\n",
      "       library time:   56.86s →   69.04s\n",
      "\n",
      "#07  sim=0.823\n",
      "     BEST query snippet #41 in input song\n",
      "       query time:   376.95s →  387.52s\n",
      "     library idx=10563, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx001264_to001296_t0203.64s_to0207.99s\n",
      "       library time:  203.64s →  207.99s\n",
      "\n",
      "#08  sim=0.822\n",
      "     BEST query snippet #41 in input song\n",
      "       query time:   376.95s →  387.52s\n",
      "     library idx=10565, song_id=285, genre=folk, file=Elvis Crespo _ Pintame.mid\n",
      "       label: folk_Elvis Crespo _ Pintame_idx001296_to001328_t0207.99s_to0212.50s\n",
      "       library time:  207.99s →  212.50s\n",
      "\n",
      "#09  sim=0.822\n",
      "     BEST query snippet #42 in input song\n",
      "       query time:   382.14s →  391.13s\n",
      "     library idx=17557, song_id=479, genre=hip-hop, file=David Bowie _ Let S Dance L.mid\n",
      "       label: hip-hop_David Bowie _ Let S Dance L_idx000128_to000160_t0026.20s_to0030.95s\n",
      "       library time:   26.20s →   30.95s\n",
      "\n",
      "#10  sim=0.818\n",
      "     BEST query snippet #13 in input song\n",
      "       query time:   113.01s →  124.10s\n",
      "     library idx=6177, song_id=154, genre=dance, file=TobyKeith_Whos_your_daddy.mid\n",
      "       label: dance_TobyKeith_Whos_your_daddy_idx000320_to000352_t0062.29s_to0068.19s\n",
      "       library time:   62.29s →   68.19s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'similarity': 0.8619452118873596,\n",
       "  'library_idx': 10567,\n",
       "  'library_song_id': 285,\n",
       "  'library_file': 'Elvis Crespo _ Pintame.mid',\n",
       "  'library_genre': 'folk',\n",
       "  'library_label': 'folk_Elvis Crespo _ Pintame_idx001328_to001360_t0212.50s_to0217.13s',\n",
       "  'library_start_sec': 212.49905867796122,\n",
       "  'library_end_sec': 217.13257887974112,\n",
       "  'query_snippet_idx': 41,\n",
       "  'query_start_sec': 376.95488721804514,\n",
       "  'query_end_sec': 387.5187969924813},\n",
       " {'rank': 2,\n",
       "  'similarity': 0.8564080595970154,\n",
       "  'library_idx': 10570,\n",
       "  'library_song_id': 285,\n",
       "  'library_file': 'Elvis Crespo _ Pintame.mid',\n",
       "  'library_genre': 'folk',\n",
       "  'library_label': 'folk_Elvis Crespo _ Pintame_idx001376_to001408_t0220.15s_to0225.25s',\n",
       "  'library_start_sec': 220.15285775472324,\n",
       "  'library_end_sec': 225.24730404987386,\n",
       "  'query_snippet_idx': 41,\n",
       "  'query_start_sec': 376.95488721804514,\n",
       "  'query_end_sec': 387.5187969924813},\n",
       " {'rank': 3,\n",
       "  'similarity': 0.8541462421417236,\n",
       "  'library_idx': 10569,\n",
       "  'library_song_id': 285,\n",
       "  'library_file': 'Elvis Crespo _ Pintame.mid',\n",
       "  'library_genre': 'folk',\n",
       "  'library_label': 'folk_Elvis Crespo _ Pintame_idx001360_to001392_t0217.13s_to0222.70s',\n",
       "  'library_start_sec': 217.13257887974112,\n",
       "  'library_end_sec': 222.7000809022986,\n",
       "  'query_snippet_idx': 41,\n",
       "  'query_start_sec': 376.95488721804514,\n",
       "  'query_end_sec': 387.5187969924813},\n",
       " {'rank': 4,\n",
       "  'similarity': 0.8347339034080505,\n",
       "  'library_idx': 10571,\n",
       "  'library_song_id': 285,\n",
       "  'library_file': 'Elvis Crespo _ Pintame.mid',\n",
       "  'library_genre': 'folk',\n",
       "  'library_label': 'folk_Elvis Crespo _ Pintame_idx001392_to001424_t0222.70s_to0227.79s',\n",
       "  'library_start_sec': 222.7000809022986,\n",
       "  'library_end_sec': 227.79452719744918,\n",
       "  'query_snippet_idx': 41,\n",
       "  'query_start_sec': 376.95488721804514,\n",
       "  'query_end_sec': 387.5187969924813},\n",
       " {'rank': 5,\n",
       "  'similarity': 0.8322111368179321,\n",
       "  'library_idx': 10564,\n",
       "  'library_song_id': 285,\n",
       "  'library_file': 'Elvis Crespo _ Pintame.mid',\n",
       "  'library_genre': 'folk',\n",
       "  'library_label': 'folk_Elvis Crespo _ Pintame_idx001280_to001312_t0206.03s_to0209.85s',\n",
       "  'library_start_sec': 206.03396373673434,\n",
       "  'library_end_sec': 209.8547984580973,\n",
       "  'query_snippet_idx': 41,\n",
       "  'query_start_sec': 376.95488721804514,\n",
       "  'query_end_sec': 387.5187969924813},\n",
       " {'rank': 6,\n",
       "  'similarity': 0.8287019729614258,\n",
       "  'library_idx': 2322,\n",
       "  'library_song_id': 61,\n",
       "  'library_file': \"You've_Lost_That_Loving_Feeling_1.mid\",\n",
       "  'library_genre': 'classic',\n",
       "  'library_label': \"classic_You've_Lost_That_Loving_Feeling_1_idx000176_to000208_t0056.86s_to0069.04s\",\n",
       "  'library_start_sec': 56.85695519506264,\n",
       "  'library_end_sec': 69.03921595669483,\n",
       "  'query_snippet_idx': 50,\n",
       "  'query_start_sec': 448.7593984962408,\n",
       "  'query_end_sec': 466.6165413533837},\n",
       " {'rank': 7,\n",
       "  'similarity': 0.8227957487106323,\n",
       "  'library_idx': 10563,\n",
       "  'library_song_id': 285,\n",
       "  'library_file': 'Elvis Crespo _ Pintame.mid',\n",
       "  'library_genre': 'folk',\n",
       "  'library_label': 'folk_Elvis Crespo _ Pintame_idx001264_to001296_t0203.64s_to0207.99s',\n",
       "  'library_start_sec': 203.64442583162796,\n",
       "  'library_end_sec': 207.98683481654209,\n",
       "  'query_snippet_idx': 41,\n",
       "  'query_start_sec': 376.95488721804514,\n",
       "  'query_end_sec': 387.5187969924813},\n",
       " {'rank': 8,\n",
       "  'similarity': 0.821955680847168,\n",
       "  'library_idx': 10565,\n",
       "  'library_song_id': 285,\n",
       "  'library_file': 'Elvis Crespo _ Pintame.mid',\n",
       "  'library_genre': 'folk',\n",
       "  'library_label': 'folk_Elvis Crespo _ Pintame_idx001296_to001328_t0207.99s_to0212.50s',\n",
       "  'library_start_sec': 207.98683481654209,\n",
       "  'library_end_sec': 212.49905867796122,\n",
       "  'query_snippet_idx': 41,\n",
       "  'query_start_sec': 376.95488721804514,\n",
       "  'query_end_sec': 387.5187969924813},\n",
       " {'rank': 9,\n",
       "  'similarity': 0.821608304977417,\n",
       "  'library_idx': 17557,\n",
       "  'library_song_id': 479,\n",
       "  'library_file': 'David Bowie _ Let S Dance L.mid',\n",
       "  'library_genre': 'hip-hop',\n",
       "  'library_label': 'hip-hop_David Bowie _ Let S Dance L_idx000128_to000160_t0026.20s_to0030.95s',\n",
       "  'library_start_sec': 26.198969084532678,\n",
       "  'library_end_sec': 30.948308266918335,\n",
       "  'query_snippet_idx': 42,\n",
       "  'query_start_sec': 382.14285714285717,\n",
       "  'query_end_sec': 391.12781954887237},\n",
       " {'rank': 10,\n",
       "  'similarity': 0.818378746509552,\n",
       "  'library_idx': 6177,\n",
       "  'library_song_id': 154,\n",
       "  'library_file': 'TobyKeith_Whos_your_daddy.mid',\n",
       "  'library_genre': 'dance',\n",
       "  'library_label': 'dance_TobyKeith_Whos_your_daddy_idx000320_to000352_t0062.29s_to0068.19s',\n",
       "  'library_start_sec': 62.290711259268484,\n",
       "  'library_end_sec': 68.18975481338401,\n",
       "  'query_snippet_idx': 13,\n",
       "  'query_start_sec': 113.00751879699247,\n",
       "  'query_end_sec': 124.09774436090224}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after training:\n",
    "# info_contrastive = train_contrastive_encoder(...)\n",
    "# contrastive_model = info_contrastive[\"model\"]\n",
    "\n",
    "demo_midi_path = \"../data/demo/ABBA.Mamma Mia K.mid\"\n",
    "\n",
    "demo_retrieval_for_midi_snippets(\n",
    "    midi_path=demo_midi_path,\n",
    "    model=contrastive_model,\n",
    "    model_type=\"contrastive\",\n",
    "    top_k=10,\n",
    "    embed_npz_path=CONTRASTIVE_EMBED_PATH,\n",
    "    snippets_npz_path=SNIPPETS_PATH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d92e6-14da-4ddd-a21f-1f75c04df5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
