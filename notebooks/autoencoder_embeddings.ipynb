{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71a9fb6-ee4d-4a14-bd18-0ba3e953ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/snippets.npz\")\n",
    "EMBED_OUT_PATH = Path(\"../data/processed/autoencoder_embeddings.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c0add2-538b-4bad-aa7b-9171ef2d9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Dataset & utilities\n",
    "# ---------------------\n",
    "\n",
    "class MelodySnippetsDataset(Dataset):\n",
    "    def __init__(self, intervals, indices):\n",
    "        \"\"\"\n",
    "        intervals: np.ndarray of shape (N, L)\n",
    "        indices: np.ndarray of snippet indices to use for this split\n",
    "        \"\"\"\n",
    "        self.intervals = intervals\n",
    "        self.indices = np.array(indices, dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        seq = self.intervals[i]  # shape (L,)\n",
    "        # return as tensor of token ids\n",
    "        return torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "\n",
    "def load_data_and_build_vocab(data_path=DATA_PATH):\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    intervals = data[\"intervals\"]  # (N, L)\n",
    "    song_ids = data[\"song_ids\"]\n",
    "    # durations = data[\"durations\"]  # not used yet\n",
    "    midi_filenames = data.get(\"midi_filenames\", None)\n",
    "\n",
    "    # Map raw interval values (can be negative) to token ids [0..V-1]\n",
    "    min_interval = intervals.min()\n",
    "    max_interval = intervals.max()\n",
    "    shifted_intervals = intervals - min_interval  # now in [0..(max-min)]\n",
    "\n",
    "    vocab_size = int(max_interval - min_interval + 1)\n",
    "\n",
    "    return shifted_intervals, song_ids, midi_filenames, min_interval, vocab_size\n",
    "\n",
    "\n",
    "def split_by_song(song_ids, train_frac=0.7, val_frac=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Split songs (by unique song_id) into train/val/test.\n",
    "    Returns three arrays of snippet indices: train_idx, val_idx, test_idx.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    unique_songs = np.unique(song_ids)\n",
    "    rng.shuffle(unique_songs)\n",
    "\n",
    "    n_songs = len(unique_songs)\n",
    "    n_train = int(train_frac * n_songs)\n",
    "    n_val = int(val_frac * n_songs)\n",
    "\n",
    "    train_songs = unique_songs[:n_train]\n",
    "    val_songs = unique_songs[n_train:n_train + n_val]\n",
    "    test_songs = unique_songs[n_train + n_val:]\n",
    "\n",
    "    def idx_for(songs_subset):\n",
    "        mask = np.isin(song_ids, songs_subset)\n",
    "        return np.where(mask)[0]\n",
    "\n",
    "    train_idx = idx_for(train_songs)\n",
    "    val_idx = idx_for(val_songs)\n",
    "    test_idx = idx_for(test_songs)\n",
    "\n",
    "    return train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e99022d-37bd-4113-8db0-1569deb175d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Autoencoder model\n",
    "# ---------------------\n",
    "\n",
    "class MelodyAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder_rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder_rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.output_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, L) token ids\n",
    "        returns: (batch, hidden_dim) embedding\n",
    "        \"\"\"\n",
    "        emb = self.embed(x)  # (B, L, E)\n",
    "        _, h_n = self.encoder_rnn(emb)  # h_n: (num_layers, B, H)\n",
    "        # take last layer\n",
    "        h_last = h_n[-1]  # (B, H)\n",
    "        return h_last\n",
    "\n",
    "    def decode(self, z, target_seq):\n",
    "        \"\"\"\n",
    "        z: (batch, H) encoder embedding\n",
    "        target_seq: (batch, L) token ids, used with teacher forcing\n",
    "        returns logits: (batch, L, vocab_size)\n",
    "        \"\"\"\n",
    "        B, L = target_seq.shape\n",
    "        # repeat z as initial hidden state\n",
    "        h0 = z.unsqueeze(0)  # (1, B, H)\n",
    "        # teacher forcing: feed ground-truth tokens as inputs\n",
    "        emb = self.embed(target_seq)  # (B, L, E)\n",
    "        out, _ = self.decoder_rnn(emb, h0)  # (B, L, H)\n",
    "        logits = self.output_fc(out)  # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Autoencoder forward: encode then decode with teacher forcing.\n",
    "        \"\"\"\n",
    "        z = self.encode(x)\n",
    "        logits = self.decode(z, x)\n",
    "        return logits, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1196c22-fde2-46c9-a188-2ca9372e002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Training loop\n",
    "# ---------------------\n",
    "\n",
    "def train_autoencoder(\n",
    "    batch_size=64,\n",
    "    num_epochs=10,\n",
    "    lr=1e-3,\n",
    "    device=None,\n",
    "):\n",
    "    intervals, song_ids, midi_filenames, min_interval, vocab_size = load_data_and_build_vocab()\n",
    "\n",
    "    train_idx, val_idx, test_idx = split_by_song(song_ids)\n",
    "\n",
    "    print(f\"Total snippets: {intervals.shape[0]}\")\n",
    "    print(f\"Train/Val/Test sizes: {len(train_idx)}, {len(val_idx)}, {len(test_idx)}\")\n",
    "    print(f\"Interval vocab size: {vocab_size}, min_interval={min_interval}\")\n",
    "\n",
    "    train_ds = MelodySnippetsDataset(intervals, train_idx)\n",
    "    val_ds = MelodySnippetsDataset(intervals, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = MelodyAutoencoder(vocab_size=vocab_size, embed_dim=64, hidden_dim=128)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # will apply over vocabulary at each timestep\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def run_epoch(loader, train=True):\n",
    "        if train:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        total_loss = 0.0\n",
    "        n_tokens = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)  # (B, L)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(train):\n",
    "                logits, _ = model(batch)  # logits: (B, L, V)\n",
    "                # reshape for CrossEntropy: (B*L, V) vs (B*L,)\n",
    "                B, L, V = logits.shape\n",
    "                loss = criterion(logits.view(B * L, V), batch.view(B * L))\n",
    "                if train:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            total_loss += loss.item() * B * L\n",
    "            n_tokens += B * L\n",
    "\n",
    "        return total_loss / n_tokens\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = run_epoch(train_loader, train=True)\n",
    "        val_loss = run_epoch(val_loader, train=False)\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "    # After training, compute embeddings for all snippets (train+val+test)\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    full_ds = MelodySnippetsDataset(intervals, np.arange(intervals.shape[0]))\n",
    "    full_loader = DataLoader(full_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in full_loader:\n",
    "            batch = batch.to(device)\n",
    "            z = model.encode(batch)  # (B, H)\n",
    "            all_embeddings.append(z.cpu().numpy())\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)  # (N, H)\n",
    "    print(\"All embeddings shape:\", all_embeddings.shape)\n",
    "\n",
    "    # Save everything we need for retrieval\n",
    "    EMBED_OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        EMBED_OUT_PATH,\n",
    "        embeddings=all_embeddings,\n",
    "        song_ids=song_ids,\n",
    "        min_interval=min_interval,\n",
    "        vocab_size=vocab_size,\n",
    "        midi_filenames=midi_filenames,\n",
    "    )\n",
    "    print(f\"Saved autoencoder embeddings to {EMBED_OUT_PATH}\")\n",
    "    print(\"Test indices (for later evaluation) lengths:\", len(test_idx))\n",
    "\n",
    "    # Return indices for convenience if called from notebook\n",
    "    return {\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "        \"min_interval\": min_interval,\n",
    "        \"vocab_size\": vocab_size,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4571b3-53c1-4735-83a1-84ac050333e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Retrieval using embeddings\n",
    "# ---------------------\n",
    "\n",
    "def load_embeddings(embed_path=EMBED_OUT_PATH):\n",
    "    data = np.load(embed_path, allow_pickle=True)\n",
    "    embeddings = data[\"embeddings\"]   # (N, H)\n",
    "    song_ids = data[\"song_ids\"]\n",
    "    midi_filenames = data.get(\"midi_filenames\", None)\n",
    "    return embeddings, song_ids, midi_filenames\n",
    "\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    a: (N, H)\n",
    "    b: (H,) or (1, H)\n",
    "    returns: (N,)\n",
    "    \"\"\"\n",
    "    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = b / np.linalg.norm(b)\n",
    "    return np.dot(a_norm, b_norm)\n",
    "\n",
    "\n",
    "def retrieve_with_embeddings(query_idx, top_k=5):\n",
    "    embeddings, song_ids, midi_filenames = load_embeddings()\n",
    "    N, H = embeddings.shape\n",
    "    if query_idx < 0 or query_idx >= N:\n",
    "        raise ValueError(f\"query_idx {query_idx} out of range [0, {N-1}]\")\n",
    "\n",
    "    q_emb = embeddings[query_idx]  # (H,)\n",
    "    sims = cosine_sim(embeddings, q_emb)  # (N,)\n",
    "\n",
    "    # Higher cosine similarity = more similar\n",
    "    sorted_idx = np.argsort(-sims)\n",
    "\n",
    "    print(f\"Query snippet {query_idx}, song_id={song_ids[query_idx]}\")\n",
    "    if midi_filenames is not None:\n",
    "        print(f\"  file={midi_filenames[song_ids[query_idx]]}\")\n",
    "\n",
    "    print(f\"\\nTop {top_k} neighbors by embedding cosine similarity:\")\n",
    "    printed = 0\n",
    "    for idx in sorted_idx:\n",
    "        if idx == query_idx:\n",
    "            continue\n",
    "        sid = int(song_ids[idx])\n",
    "        fname = midi_filenames[sid] if midi_filenames is not None else \"N/A\"\n",
    "        print(f\"  idx={idx:4d}, song_id={sid}, file={fname}, sim={sims[idx]:.3f}\")\n",
    "        printed += 1\n",
    "        if printed >= top_k:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c885e2-bb07-4399-813f-7513a95c56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from autoencoder_embeddings import train_autoencoder, retrieve_with_embeddings\n",
    "\n",
    "# Train (or just run once and re-use saved embeddings)\n",
    "info = train_autoencoder(num_epochs=5, batch_size=64, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148601b5-52d0-410b-b471-3f7cb48837b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_with_embeddings(query_idx=10, top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs-441-project)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
